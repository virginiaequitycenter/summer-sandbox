{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landsat 8 Surface Temperature Generative Script\n",
    "Chase Dawson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Read in Spatial Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/chasedawson/dev/uva_equity_center/climate_equity'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load libraries\n",
    "import json\n",
    "import requests\n",
    "from dotenv import dotenv_values\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from shapely.geometry import mapping\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import earthpy as et\n",
    "import earthpy.plot as ep\n",
    "from shapely.geometry import Polygon\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# print cwd\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_spatial_data(basename, clip_coast=False):\n",
    "    \"\"\"\n",
    "    Reads in spatial data that is in the format 'basename_spatialUnit.shp'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    basename : str, required\n",
    "        Base name of shape files. For example, if the files you want to read are of the format\n",
    "        'cville_counties.shp', 'cville_blocks.shp', etc. then the basename is 'cville'.\n",
    "        \n",
    "    clip_coast : bool, optional (default is False)\n",
    "        If True, shp data will be compared to coast line data and oceans will be clipped out.\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    dictionary containing geopandas dataframes for each spatial unit\n",
    "    \n",
    "    \"\"\"\n",
    "    print('Reading spatial data for {basename}...'.format(basename = basename))\n",
    "    # read in coast line data \n",
    "    if clip_coast:\n",
    "        ocean = gpd.read_file('water/ne_10m_ocean.shp')\n",
    "    \n",
    "    # store current working directory\n",
    "    og_wd = os.getcwd()\n",
    "    \n",
    "    # change working directory to where spatial data is located\n",
    "    os.chdir(\"../spatial_units/data\")\n",
    "    \n",
    "    # create empty dictionary\n",
    "    data = {}\n",
    "    \n",
    "    if clip_coast:\n",
    "        spatial_units = ['blocks', 'blkgps', 'tracts']\n",
    "\n",
    "        # read in counties and clip first\n",
    "        print(\"Reading counties...\")\n",
    "        counties = gpd.read_file(basename + '_counties.shp')\n",
    "        ocean = ocean.to_crs(counties.crs)\n",
    "        counties = gpd.overlay(counties, ocean, how='difference')\n",
    "        data['counties'] = counties\n",
    "        print(\"Done.\")\n",
    "    \n",
    "        # read in rest of shapefiles, clip respective to counties, store as keys in dict\n",
    "        for spatial_unit in spatial_units:\n",
    "            print(\"Reading {spatial_unit}...\".format(spatial_unit = spatial_unit))\n",
    "            # read in shp file\n",
    "            shp = gpd.read_file(basename + '_{spatial_unit}.shp'.format(spatial_unit = spatial_unit))\n",
    "\n",
    "            # convert coast line data to crs of shp \n",
    "            counties = counties.to_crs(shp.crs)\n",
    "\n",
    "            # clip out ocean\n",
    "            shp = gpd.overlay(shp, counties[['geometry']], how='intersection', keep_geom_type=True)\n",
    "\n",
    "            # add dict with spatial unit as key\n",
    "            data[spatial_unit] = shp\n",
    "            print(\"Done.\")\n",
    "            \n",
    "    else:\n",
    "        spatial_units = ['counties', 'blocks', 'blkgps', 'tracts']\n",
    "        for spatial_unit in spatial_units:\n",
    "            print(\"Reading {spatial_unit}...\".format(spatial_unit = spatial_unit))\n",
    "            shp = gpd.read_file(basename + '_{spatial_unit}.shp'.format(spatial_unit = spatial_unit))\n",
    "            data[spatial_unit] = shp\n",
    "            print(\"Done.\")\n",
    "\n",
    "    # reset back to original working directory\n",
    "    os.chdir(og_wd)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading spatial data for cville...\n",
      "Reading counties...\n",
      "Done.\n",
      "Reading blocks...\n",
      "Done.\n",
      "Reading blkgps...\n",
      "Done.\n",
      "Reading tracts...\n",
      "Done.\n",
      "\n",
      "\n",
      "Reading spatial data for eastshore...\n",
      "Reading counties...\n",
      "Done.\n",
      "Reading blocks...\n",
      "Done.\n",
      "Reading blkgps...\n",
      "Done.\n",
      "Reading tracts...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# read in shp files\n",
    "cville_shp = read_spatial_data('cville')\n",
    "print('\\n')\n",
    "easternShore_shp = read_spatial_data('eastshore', clip_coast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounds(gdf):\n",
    "    \"\"\"\n",
    "    Get lower left and upper right (lat, lng) coordinates for all geometries\n",
    "    in GeoDataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf : GeoDataFrame, required\n",
    "        GeoPandas DataFrame that you want to get a bounding box for.\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    Dictionary containing lower left and upper right (lat, lng) coordinates\n",
    "    of the bounding box. \n",
    "    \n",
    "    \"\"\"\n",
    "    bounds_df = gdf.bounds\n",
    "    lowerLeft = {'latitude': round(bounds_df['miny'].min(), 6), 'longitude': round(bounds_df['minx'].min(), 6)}\n",
    "    upperRight = {'latitude': round(bounds_df['maxy'].max(), 6), 'longitude': round(bounds_df['maxx'].max(), 6)}\n",
    "    return {\n",
    "        'lowerLeft': lowerLeft,\n",
    "        'upperRight': upperRight\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use shpfiles to get bounding boxes for Charlottesville and Eastern Shore\n",
    "cville_bounds = get_bounds(cville_shp['counties'])\n",
    "easternShore_bounds = get_bounds(easternShore_shp['counties'])\n",
    "\n",
    "# get perimeter shps\n",
    "cville_perimeter = cville_shp['counties'].dissolve()\n",
    "easternShore_perimeter = easternShore_shp['counties'].dissolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Landsat 8 Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Methods for working with USGS API ##\n",
    "\n",
    "# API base URL\n",
    "SERVICE_URL = \"https://m2m.cr.usgs.gov/api/api/json/stable/\"\n",
    "\n",
    "def login(username, password):\n",
    "    \"\"\"\n",
    "    Authenticates user given username and password and returns API key.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    username : str, required\n",
    "        USGS account username.\n",
    "        \n",
    "    password : str, required\n",
    "        USGS account password. \n",
    "        \n",
    "    Notes \n",
    "    -----\n",
    "    Go to https://ers.cr.usgs.gov/profile/access to request access \n",
    "    to the API and/or make an account.\n",
    "    \n",
    "    \"\"\"\n",
    "    # login information\n",
    "    payload = {'username': username, 'password': password}\n",
    "\n",
    "    # get apiKey \n",
    "    apiKey = sendRequest(SERVICE_URL + \"login\", payload)\n",
    "    if apiKey == None:\n",
    "        print(\"Login Failed\\n\\n\")\n",
    "    else:\n",
    "        print(\"Login Successful\\n\\n\")\n",
    "    \n",
    "    return apiKey\n",
    "\n",
    "def logout(apiKey):\n",
    "    \"\"\"\n",
    "    Invalidates API key. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    apiKey : str, required\n",
    "        Valid API key. Obtain using the login() method defined above.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Make sure to call when you've finished working to ensure that your \n",
    "    API key can't be used by an unauthorized user.\n",
    "    \n",
    "    \"\"\"\n",
    "    endpoint = \"logout\"\n",
    "    if sendRequest(SERVICE_URL + endpoint, None, apiKey) == None:\n",
    "        print(\"Logged Out\\n\\n\")\n",
    "    else:\n",
    "        print(\"Logout Failed\\n\\n\")\n",
    "\n",
    "def sendRequest(url, data, apiKey = None):\n",
    "    \"\"\"\n",
    "    Sends HTTPS request to specified API endpoint. Main method for interacting\n",
    "    with the API.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str, required\n",
    "        API endpoint you wish you access. Typical format is SERVICE_URL + endpoint, \n",
    "        where endpoint might be something like \"login\" or \"data-search.\" See https://m2m.cr.usgs.gov/api/docs/reference/\n",
    "        for all available endpoints.\n",
    "        \n",
    "    data : dict, required\n",
    "        Request payload. Data required changes based on API endpoint. See \n",
    "        https://m2m.cr.usgs.gov/api/docs/reference/ for input parameters, sample requests,\n",
    "        sample and responses for available endpoints.\n",
    "        \n",
    "    apiKey : str, optional (default is None)\n",
    "        Valid API key. Must be speficied for most requests. \"login\" endpoint doesn't \n",
    "        require an API key since you use that endpoint to retrieve a valid API key.\n",
    "    \n",
    "    \"\"\"\n",
    "    json_data = json.dumps(data)\n",
    "    \n",
    "    if apiKey == None:\n",
    "        response = requests.post(url, json_data)\n",
    "    else:\n",
    "        headers = {'X-Auth-Token': apiKey}\n",
    "        response = requests.post(url, json_data, headers = headers)\n",
    "          \n",
    "    try:\n",
    "        httpStatusCode = response.status_code\n",
    "        \n",
    "        if response == None:\n",
    "            print(\"No output from service!\")\n",
    "            sys.exit()\n",
    "            \n",
    "        output = json.loads(response.text)\n",
    "        if output['errorCode'] != None:\n",
    "            print(output['errorCode'], \"- \", output['errorMessage'])\n",
    "            sys.exit()\n",
    "            \n",
    "        if httpStatusCode == 404:\n",
    "            print(\"404 Not Found\")\n",
    "            sys.exit()\n",
    "            \n",
    "        elif httpStatusCode == 401:\n",
    "            print(\"401 Unauthorized\")\n",
    "            sys.exit()\n",
    "            \n",
    "        elif httpStatusCode == 400:\n",
    "            print(\"Error Code\", httpStatusCode)\n",
    "            sys.exit()\n",
    "            \n",
    "    except Exception as e:\n",
    "        response.close()\n",
    "        print(e)\n",
    "        sys.exit()\n",
    "    \n",
    "    response.close()\n",
    "    return output['data']\n",
    "\n",
    "def getFilename_fromCd(cd):\n",
    "    \"\"\"\n",
    "    Uses content-disposition to infer filename and filetype.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cd : str, required\n",
    "        The Content-Disposition response header from HTTP request \n",
    "        to download a file.\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    Inferred filename and type of provided file : str  \n",
    "    \"\"\"\n",
    "    if not cd:\n",
    "        return None\n",
    "    fname = re.findall('filename=(.+)', cd)\n",
    "    if len(fname) == 0:\n",
    "        return None\n",
    "    \n",
    "    return re.sub('\\\"', '', fname[0]) # remove extra quotes\n",
    "\n",
    "def download_file(url):\n",
    "    \"\"\"\n",
    "    Saves file to local system.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        url: str, required\n",
    "            Link to file to be downloaded.\n",
    "            \n",
    "    Output\n",
    "    ------\n",
    "    Path to downloaded file : str\n",
    "    \"\"\"\n",
    "    res = requests.get(url)\n",
    "    filename = getFilename_fromCd(res.headers.get('content-disposition'))\n",
    "    open(filename, 'wb').write(res.content)\n",
    "    return filename\n",
    "    \n",
    "def search_scenes(apiKey, bounds, start_date, end_date, dataset = \"landsat_ot_c2_l2\", cloud_cover_min = 0, cloud_cover_max = 10):\n",
    "    \"\"\"\n",
    "    Search specified dataset for scenes given spatial and temporal filters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    apiKey : str, required\n",
    "        Valid API key.\n",
    "        \n",
    "    bounds: dict, required\n",
    "        Dictionary with two entries: 'lowerLeft' and 'upperRight' which contain\n",
    "        the lower left and upper right lat, lng coordinates of the bounding box covering\n",
    "        the area of interest.\n",
    "        \n",
    "    start_date: str, required\n",
    "        Format: YYYY-MM-DD\n",
    "        \n",
    "    end_date: str, required\n",
    "        Format: YYYY-MM-DD\n",
    "        \n",
    "    dataset: str, optional (default is 'landsat_ot_c2_l2')\n",
    "        Dataset alias. Use the 'dataset-search' endpoint to discover\n",
    "        which datasets are available.\n",
    "        \n",
    "    cloud_cover_min : int, optional (default is 0)\n",
    "        Minimum cloud coverage percentage. Scenes with cloud coverage less\n",
    "        than this value will not be included in the result.\n",
    "        \n",
    "    cloud_cover_max: int, optional (default is 10)\n",
    "        \n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        'datasetName': dataset,\n",
    "        'startingNumber': 1,\n",
    "        'sceneFilter': {\n",
    "            'spatialFilter': {\n",
    "                'filterType': 'mbr',\n",
    "                'lowerLeft': bounds['lowerLeft'],\n",
    "                'upperRight': bounds['upperRight']\n",
    "            },\n",
    "            'acquisitionFilter': {\n",
    "                'start': start_date,\n",
    "                'end': end_date\n",
    "            },\n",
    "            'cloudCoverFilter': {\n",
    "                'max': 10,\n",
    "                'min': 0,\n",
    "                'includeUnknown': False,\n",
    "            },\n",
    "            'seasonalFilter': [6,7,8,9]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"Searching Scenes...\")\n",
    "    scenes = sendRequest(SERVICE_URL + \"scene-search\", payload, apiKey)\n",
    "    print(\"Found {num_scenes} Scene(s).\".format(num_scenes = scenes['recordsReturned']))\n",
    "    \n",
    "    return scenes\n",
    "\n",
    "def get_acquisitionDates(scenes):\n",
    "    acquisitionDates = {}\n",
    "    for result in scenes['results']:\n",
    "        entityId = result['entityId']\n",
    "        metadata = result['metadata']\n",
    "        for field in metadata:\n",
    "            if field['fieldName'] == \"Acquisition Date\":\n",
    "                date = field['value']\n",
    "                acquisitionDates[entityId] = date\n",
    "                break\n",
    "    return acquisitionDates\n",
    "\n",
    "\n",
    "def get_scene_metadata(scenes):\n",
    "    scene_metadata = []\n",
    "    for result in scenes['results']:\n",
    "        entityId = result['entityId']\n",
    "        acquisitionDate = None\n",
    "        metadata = result['metadata']\n",
    "        for field in metadata:\n",
    "            if field['fieldName'] == \"Acquisition Date\":\n",
    "                acquisitionDate = field['value']\n",
    "                break\n",
    "        cloudCover = result['cloudCover']\n",
    "        publishDate = result['publishDate']\n",
    "        startDate = result['temporalCoverage']['startDate']\n",
    "        endDate = result['temporalCoverage']['endDate']\n",
    "        spatialCoverage = result['spatialCoverage']['coordinates'][0]\n",
    "        spatialBounds = result['spatialBounds']['coordinates'][0]\n",
    "        polygon_geom = Polygon([(x[0], x[1]) for x in spatialCoverage])\n",
    "        new_row = {'entity_id': entityId, 'acquisition_date': acquisitionDate, 'publish_date': publishDate, 'start_date': startDate,\n",
    "                   'end_date': endDate, 'cloud_cover': cloudCover, 'spatial_bounds': spatialBounds, 'spatial_coverage': spatialCoverage,\n",
    "                  'geometry': polygon_geom}        \n",
    "        scene_metadata.append(new_row)\n",
    "    gdf = gpd.GeoDataFrame(scene_metadata, crs=\"EPSG:4326\")\n",
    "    \n",
    "    # convert date-like cols to date cols\n",
    "    date_cols = ['acquisition_date', 'publish_date', 'start_date', 'end_date']\n",
    "    for date_col in date_cols:\n",
    "        gdf[date_col] = gdf[date_col].apply(lambda x: None if x == \"Unknown\" else x)\n",
    "        gdf[date_col] = gdf[date_col].apply(lambda x: datetime.fromisoformat(x.split('.')[0]) if x is not None else x)\n",
    "    \n",
    "    # create year col\n",
    "    gdf['start_year'] = gdf.start_date.apply(lambda x: x.year)\n",
    "    gdf['end_year'] = gdf.end_date.apply(lambda x: x.year)\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "def get_area(shp):\n",
    "    # get area of shp in km^2\n",
    "    return round(shp.geometry.to_crs(\"EPSG:3395\").map(lambda p: p.area / 10**6).iloc[0], 6)\n",
    "        \n",
    "def intersection_stats(shp, scene):\n",
    "    intersection = gpd.overlay(shp, scene, how='intersection')\n",
    "    if len(intersection) == 0:\n",
    "        return {'area': 0, 'shp_percent': 0}\n",
    "    \n",
    "    # get area of intersection in km^2\n",
    "    intersect_area = get_area(intersection)\n",
    "    \n",
    "    # get area of original shp in km^2\n",
    "    shp_area = get_area(shp)\n",
    "    \n",
    "    # compute percentage of intersection of shp\n",
    "    percentage = (intersect_area / shp_area) * 100\n",
    "    \n",
    "    return {'area': intersect_area, 'shp_percent': percentage}\n",
    "    \n",
    "\n",
    "def filter_scenes(scenes, shp_data):\n",
    "    # total number of counties that could be captured by \n",
    "    total_geoms = len(shp_data)\n",
    "    \n",
    "        \n",
    "def get_sceneIds(scenes):\n",
    "    \"\"\"\n",
    "    Parses scene data to return list of scene ids.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    scenes : object, required\n",
    "        Output from search_scenes().\n",
    "        \n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    scene ids : list\n",
    "    \n",
    "    \"\"\"\n",
    "    sceneIds = []\n",
    "    for result in scenes['results']:\n",
    "        sceneIds.append(result['entityId'])\n",
    "    return sceneIds\n",
    "\n",
    "def download_scenes(apiKey, sceneIds, label, dataset = \"landsat_ot_c2_l2\"):\n",
    "    \"\"\"\n",
    "    Downloads scenes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    apiKey : str, required\n",
    "        Valid API key.\n",
    "        \n",
    "    scenes : object, required\n",
    "        Scenes you wish to download. Returned from search_scenes().\n",
    "        \n",
    "    label : str, required\n",
    "        Label for your download request.\n",
    "        \n",
    "    dataset : str, optional (default is 'landsat_ot_c2_l2')\n",
    "        Must be the dataset the scenes are from. \n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    Paths to downloaded files : list\n",
    "    \"\"\"\n",
    "        \n",
    "    # download options\n",
    "    payload = {\n",
    "        'datasetName': dataset,\n",
    "        'entityIds': sceneIds,\n",
    "    }\n",
    "    \n",
    "    downloadOptions = sendRequest(SERVICE_URL + \"download-options\", payload, apiKey)\n",
    "    \n",
    "    # aggregate list of available products\n",
    "    downloads = []\n",
    "    for product in downloadOptions:\n",
    "        # make sure the product is available for this scene\n",
    "        if product['available'] == True:\n",
    "            downloads.append({'entityId': product['entityId'],\n",
    "                             'productId': product['id']})\n",
    "            \n",
    "    if downloads:\n",
    "        requestedDownloadsCount = len(downloads)\n",
    "        print(\"Number of Requested Downloads: {requestedDownloadsCount}\".format(requestedDownloadsCount = requestedDownloadsCount))\n",
    "        print(\"Downloading Now...\")\n",
    "        payload = {\n",
    "            'downloads': downloads,\n",
    "            'label': label\n",
    "        }\n",
    "        requestResults = sendRequest(SERVICE_URL + \"download-request\", payload, apiKey)\n",
    "        if requestResults['preparingDownloads'] != None and len(requestResults['preparingDownloads']) > 0:\n",
    "            payload = {'label': label}\n",
    "            downloadUrls = sendRequest(SERVICE_URL + \"download-retrieve\", payload, apiKey)\n",
    "            downloadIds = []\n",
    "            for download in downloadUrls['available']:\n",
    "                downloadIds.append(download['downloadId'])\n",
    "                \n",
    "            for download in downloadUrls['requested']:\n",
    "                downloadIds.append(download['downloadId'])\n",
    "                \n",
    "            while len(downloadIds) < requestedDownloadsCount:\n",
    "                preparingDownloads = requestedDownloadsCount - len(downloadIds)\n",
    "                print('\\n', preparingDownloads, \"download(s) are not yet available. Waiting for 30 seconds.\\n\")\n",
    "                time.sleep(30)\n",
    "                print(\"Trying to retrieve data.\\n\")\n",
    "                downloadUrls = sendRequest(SERVICE_URL + \"download-retrieve\", payload, apiKey)\n",
    "                for download in downloadUrls['available']:\n",
    "                    if download['downloadId'] not in downloadIds:\n",
    "                        downloadIds.append(download['downloadId'])\n",
    "        else:\n",
    "            # get all available downloads\n",
    "            # search requested downloads to get metadata\n",
    "            payload = {\n",
    "                'label': label\n",
    "            }\n",
    "            download_search = sendRequest(SERVICE_URL + \"download-search\", payload, apiKey)\n",
    "            download_metadata = {}\n",
    "            for search_result in download_search:\n",
    "                entityId = search_result['entityId']\n",
    "                displayId = search_result['displayId']\n",
    "                downloadId = search_result['downloadId']\n",
    "                download_metadata[downloadId] = {'entityId': entityId, 'displayId': displayId}\n",
    "                \n",
    "            files = []\n",
    "            for download in requestResults['availableDownloads']:\n",
    "                url = download['url']\n",
    "                downloadId = download['downloadId']\n",
    "                filename = download_file(url)\n",
    "                print(\"{filename} Downloaded Successfully.\".format(filename = filename))\n",
    "                filedata = {\n",
    "                    'entityId': download_metadata[downloadId]['entityId'],\n",
    "                    'displayId': download_metadata[downloadId]['displayId'],\n",
    "                    'filename': filename\n",
    "                }\n",
    "                files.append(filedata)\n",
    "                \n",
    "            print(\"\\nAll files have been downloaded.\\n\")\n",
    "            return files\n",
    "        \n",
    "    else:\n",
    "        print(\"No available products.\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Successful\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# log in to retrieve API key\n",
    "config = dotenv_values('.env')\n",
    "apiKey = login(config['USGS_USERNAME'], config['USGS_PASSWORD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"landsat_ot_c2_l2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define temporal filter start and end constants\n",
    "SEARCH_START = \"2016-06-00\"\n",
    "SEARCH_END = \"2020-09-30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersection_stats(shp, metadata):\n",
    "    area = []\n",
    "    shp_percent = []\n",
    "    for i in range(len(metadata)):\n",
    "        stats = intersection_stats(shp, metadata.iloc[[i]])\n",
    "        area.append(stats['area'])\n",
    "        shp_percent.append(stats['shp_percent'])\n",
    "    metadata['area'] = area\n",
    "    metadata['shp_percent'] = shp_percent\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_scenes(metadata, shp):\n",
    "    # filter scenes with 2 constraints:\n",
    "    # 1. each year is represented (or number of years represented is maximized)\n",
    "    # 2. as much of shp is covered as possible\n",
    "    \n",
    "    to_keep = [] # list of entity ids of scenes to keep \n",
    "    \n",
    "    years = metadata.start_year.value_counts().index.tolist()\n",
    "    years.sort()\n",
    "    # iterate over each year \n",
    "    for year in years:\n",
    "        metadata_yr = metadata[metadata.start_year == year]\n",
    "        \n",
    "        # sort by shp_percentage\n",
    "        metadata_yr = metadata_yr.sort_values(by=[\"shp_percent\", \"cloud_cover\"], ascending=False)\n",
    "        \n",
    "        shp_left = shp # keep track of what part of shp hasn't been covered yet\n",
    "        \n",
    "        # iterate over scenes in year, starting with the scene that has the largest intersection with shp\n",
    "        for i in range(len(metadata_yr)):\n",
    "            scene = metadata_yr.iloc[[i]].to_crs(shp.crs) # load scene and convert to crs of shp\n",
    "            intersection = gpd.overlay(shp_left, scene, how='intersection')\n",
    "            stats = intersection_stats(shp_left, scene)\n",
    "            intersected = len(intersection) > 0\n",
    "            if stats['shp_percent'] == 100.0:\n",
    "                # add scene to keep \n",
    "                to_keep.append(scene.iloc[0]['entity_id'])\n",
    "                break\n",
    "                \n",
    "            elif intersected:\n",
    "                # add scene to keep\n",
    "                to_keep.append(scene.iloc[0]['entity_id'])\n",
    "                \n",
    "                # update shp_left\n",
    "                shp_left = intersection\n",
    "                    \n",
    "    # filter metadata to entity ids in to_keep and return\n",
    "    return metadata[metadata.entity_id.isin(to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Scenes...\n",
      "Found 49 Scene(s).\n",
      "Filtered down to 27 Scene(s).\n"
     ]
    }
   ],
   "source": [
    "# get data for Charlottesville\n",
    "cville_scenes = search_scenes(apiKey, cville_bounds, SEARCH_START, SEARCH_END)\n",
    "cville_scene_metadata = get_scene_metadata(cville_scenes)\n",
    "\n",
    "# convert metadata to crs of cville spatial data\n",
    "cville_scene_metadata = cville_scene_metadata.to_crs(cville_shp['counties'].crs)\n",
    "cville_scene_metadata = get_intersection_stats(cville_shp['counties'].dissolve(), cville_scene_metadata)\n",
    "\n",
    "# filter\n",
    "cville_scene_metadata_f = filter_scenes(cville_scene_metadata, cville_shp['counties'].dissolve())\n",
    "\n",
    "print(\"Filtered down to {num_scenes} Scene(s).\".format(num_scenes = len(cville_scene_metadata_f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Scenes...\n",
      "Found 58 Scene(s).\n",
      "Filtered down to 5 Scene(s).\n"
     ]
    }
   ],
   "source": [
    "# search and filter scenes for the Eastern Shore\n",
    "easternShore_scenes = search_scenes(apiKey, easternShore_bounds, SEARCH_START, SEARCH_END)\n",
    "easternShore_scene_metadata = get_scene_metadata(easternShore_scenes)\n",
    "\n",
    "# convert metadata to crs of eastern shore spatial data\n",
    "easternShore_scene_metadata = easternShore_scene_metadata.to_crs(easternShore_shp['counties'].crs)\n",
    "easternShore_scene_metadata = get_intersection_stats(easternShore_shp['counties'].dissolve(), easternShore_scene_metadata)\n",
    "\n",
    "# filter\n",
    "easternShore_scene_metadata_f = filter_scenes(easternShore_scene_metadata, easternShore_shp['counties'].dissolve())\n",
    "print(\"Filtered down to {num_scenes} Scene(s).\".format(num_scenes = len(easternShore_scene_metadata_f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Requested Downloads: 7\n",
      "Downloading Now...\n",
      "LC08_L2SP_015034_20200713_20200912_02_T1.tar Downloaded Successfully.\n",
      "LC08_L2SP_016033_20200906_20200918_02_T1.tar Downloaded Successfully.\n",
      "LC08_L2SP_016033_20200922_20201005_02_T1.tar Downloaded Successfully.\n",
      "LC08_L2SP_016034_20200704_20200913_02_T1.tar Downloaded Successfully.\n",
      "LC08_L2SP_016034_20200720_20210330_02_T1.tar Downloaded Successfully.\n",
      "LC08_L2SP_016034_20200906_20200918_02_T1.tar Downloaded Successfully.\n",
      "LC08_L2SP_016034_20200922_20201005_02_T1.tar Downloaded Successfully.\n",
      "\n",
      "All files have been downloaded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download cville data (just for 2020)\n",
    "cville_filedata = download_scenes(\n",
    "    apiKey,\n",
    "    cville_scene_metadata_f[cville_scene_metadata_f.start_year == 2020].entity_id.tolist(),\n",
    "    'cville_summer_2020'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LC08_L2SP_015034_20200713_20200912_02_T1.tar',\n",
       " 'LC08_L2SP_016033_20200906_20200918_02_T1.tar',\n",
       " 'LC08_L2SP_016033_20200922_20201005_02_T1.tar',\n",
       " 'LC08_L2SP_016034_20200704_20200913_02_T1.tar',\n",
       " 'LC08_L2SP_016034_20200720_20210330_02_T1.tar',\n",
       " 'LC08_L2SP_016034_20200906_20200918_02_T1.tar',\n",
       " 'LC08_L2SP_016034_20200922_20201005_02_T1.tar']"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cville_files = [x['filename'] for x in cville_filedata]\n",
    "cville_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Requested Downloads: 1\n",
      "Downloading Now...\n",
      "LC08_L2SP_014034_20200722_20200911_02_T1.tar Downloaded Successfully.\n",
      "\n",
      "All files have been downloaded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "easternShore_filedata = download_scenes(\n",
    "    apiKey, \n",
    "    easternShore_scene_metadata_f[easternShore_scene_metadata_f.start_year == 2020].entity_id.tolist(),\n",
    "    'easternShore_summer_2020'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LC08_L2SP_014034_20200722_20200911_02_T1.tar']"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easternShore_files = [x['filename'] for x in easternShore_filedata]\n",
    "easternShore_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged Out\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logout of API\n",
    "# if successful, apiKey is now invalid\n",
    "logout(apiKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/chasedawson/dev/uva_equity_center/climate_equity'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make directories where data will be extracted\n",
    "os.mkdir(\"landsat8_c2_l2_data\")\n",
    "os.chdir(\"landsat8_c2_l2_data\")\n",
    "os.mkdir(\"cville\")\n",
    "os.mkdir(\"easternShore\")\n",
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "def extract_tar(tar, extract_to):\n",
    "    \"\"\"\n",
    "    Extracts tar file to specified location.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tar : str, required\n",
    "        Path to tar file that will be extracted.\n",
    "        \n",
    "    extract_to : str, required\n",
    "        Path to folder in which the tar file will be extracted.\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\"Extracting\", tar, \"...\")\n",
    "    my_tar = tarfile.open(tar)\n",
    "    my_tar.extractall(extract_to)\n",
    "    my_tar.close()\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths for extracted tar files for both geographic regions\n",
    "# of interest\n",
    "cville_tar_path = \"./landsat8_c2_l2_data/cville\"\n",
    "easternShore_tar_path = \"./landsat8_c2_l2_data/easternShore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting LC08_L2SP_016034_20200922_20201005_02_T1.tar ...\n",
      "Done.\n",
      "Extracting LC08_L2SP_016034_20200906_20200918_02_T1.tar ...\n",
      "Done.\n",
      "Extracting LC08_L2SP_016034_20200720_20210330_02_T1.tar ...\n",
      "Done.\n",
      "Extracting LC08_L2SP_016034_20200704_20200913_02_T1.tar ...\n",
      "Done.\n",
      "Extracting LC08_L2SP_016033_20200922_20201005_02_T1.tar ...\n",
      "Done.\n",
      "Extracting LC08_L2SP_016033_20200906_20200918_02_T1.tar ...\n",
      "Done.\n",
      "Extracting LC08_L2SP_015034_20200713_20200912_02_T1.tar ...\n",
      "Done.\n",
      "Extracting LC08_L2SP_014034_20200722_20200911_02_T1.tar ...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# extract cville files and easternShore files\n",
    "for tar in cville_files:\n",
    "    extract_tar(tar, cville_tar_path)\n",
    "    \n",
    "for tar in easternShore_files:\n",
    "    extract_tar(tar, easternShore_tar_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete original tar files\n",
    "for tar in cville_files:\n",
    "    os.remove(tar)\n",
    "    \n",
    "for tar in easternShore_files:\n",
    "    os.remove(tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tar_endings(files):\n",
    "    \"\"\"\n",
    "    Removes .tar endings from a list of filenames.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list, required\n",
    "        List of tar filenames.\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    list of filenames with .tar endings removed : list\n",
    "    \n",
    "    \"\"\"\n",
    "    reformatted_names = []\n",
    "    for file in files:\n",
    "        reformatted_names.append(file.split('.')[0])\n",
    "    return reformatted_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove .tar endings\n",
    "cville_files = remove_tar_endings(cville_files)\n",
    "easternShore_files = remove_tar_endings(easternShore_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip Raster Data with Shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename ending for raster temperature data\n",
    "ST_ENDING = \"_ST_B10.TIF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_crs(shp_data, raster_src):\n",
    "    for key in shp_data:\n",
    "        shp_data[key] = shp_data[key].to_crs(raster_src.crs)\n",
    "    return shp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make clipped dirs in both cville and easternShore\n",
    "os.mkdir(\"landsat8_c2_l2_data/cville/clipped\")\n",
    "os.mkdir(\"landsat8_c2_l2_data/easternShore/clipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_rasters(files, shp_data, basename):\n",
    "    \"\"\"\n",
    "    Clip set of rasters given shapefiles. Saves these files in\n",
    "    'landsat8_c2_l2_data/basename/clipped'. With the geographic region and\n",
    "    spatial resolution encoded in the name.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list, required\n",
    "        List of raster file base names to be clipped.\n",
    "    \n",
    "    shp_data : dict, required\n",
    "        Geometry data for given region. A dictionary containing geometries for different spatial resolutions.\n",
    "        \n",
    "    basename : str, required\n",
    "        Name of geographic region. Either 'cville' or 'easternShore'.\n",
    "    \"\"\"\n",
    "    full_boundary = shp_data['counties'].dissolve()\n",
    "    for file in files:\n",
    "        path = 'landsat8_c2_l2_data/{basename}/{file}{ending}'.format(basename = basename, file = file, ending = ST_ENDING)\n",
    "        with rio.open(path) as src:\n",
    "            full_boundary = full_boundary.to_crs(src.crs)\n",
    "        raster_data = rxr.open_rasterio(path, masked=True).squeeze()\n",
    "        raster_clipped = raster_data.rio.clip(full_boundary.geometry.apply(mapping), full_boundary.crs)\n",
    "        # export clipped raster\n",
    "        new_filename = \"{file}_ST_B10_{basename}_clipped.TIF\".format(basename = basename, file = file)\n",
    "        raster_clipped.rio.to_raster(os.path.join(\"landsat8_c2_l2_data\", basename, \"clipped\", new_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip and save rasters for cville and eastern shore\n",
    "clip_rasters(cville_files, cville_shp, \"cville\")\n",
    "clip_rasters(easternShore_files, easternShore_shp, \"easternShore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from rasterio.plot import plotting_extent\n",
    "\n",
    "def plot_clipped_raster(file, shp, basename):\n",
    "    path = 'landsat8_c2_l2_data/{basename}/clipped/{file}_ST_B10_{basename}_clipped.TIF'.format(basename = basename, file = file)\n",
    "    with rio.open(path) as src:\n",
    "        st_data = src.read()\n",
    "        nodata = src.nodata\n",
    "        \n",
    "        # change type of st_data to float64 to include nan\n",
    "        st_data = st_data.astype('float64')\n",
    "        st_data[st_data == nodata] = np.nan\n",
    "        \n",
    "        shp = shp.to_crs(src.crs)\n",
    "        f, ax = plt.subplots()\n",
    "        # ax.imshow(st_data[0], cm.inferno)\n",
    "        ep.plot_bands(st_data, ax=ax, extent=plotting_extent(src), cmap=cm.inferno)\n",
    "        shp.boundary.plot(ax=ax)\n",
    "        plt.show()\n",
    "\n",
    "# plot clipped rasters and shp files\n",
    "def plot_clipped_rasters(files, shp_data, basename):\n",
    "    for file in files:\n",
    "        path = 'landsat8_c2_l2_data/{basename}/clipped/{file}_ST_B10_{basename}_clipped.TIF'.format(basename = basename, file = file)\n",
    "        with rio.open(path) as src:\n",
    "            st_data = src.read()\n",
    "            nodata = src.nodata\n",
    "            \n",
    "            # change type of st_data to float64 to include nan\n",
    "            st_data = st_data.astype('float64')\n",
    "            st_data[st_data == nodata] = np.nan\n",
    "            \n",
    "            for key in shp_data:\n",
    "                shp = shp_data[key]\n",
    "                shp = shp.to_crs(src.crs)\n",
    "                f, ax = plt.subplots()\n",
    "                ax.imshow(st_data[0], cm.inferno)\n",
    "                shp.boundary.plot(ax=ax)\n",
    "                ax.set_title('{basename} {key}'.format(basename = basename, key = key))\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zonal Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterstats import zonal_stats\n",
    "import pandas as pd\n",
    "\n",
    "# read in clipped files\n",
    "def compute_zonal_stats(files, shp_data, basename):\n",
    "    stats = pd.DataFrame()\n",
    "    for file in files:\n",
    "        print(file)\n",
    "        path = 'landsat8_c2_l2_data/{basename}/clipped/{file}_ST_B10_{basename}_clipped.TIF'.format(basename = basename, file = file)\n",
    "        with rio.open(path) as src:\n",
    "            affine = src.transform\n",
    "            nodata = src.profile['nodata']\n",
    "            raster_data = src.read(1)\n",
    "            for key in shp_data:\n",
    "                gdf = shp_data[key]\n",
    "                gdf = gdf.to_crs(src.crs)\n",
    "                # plot_clipped_raster(file, gdf, basename)\n",
    "                for i in range(len(gdf)):\n",
    "                    geoId = gdf.iloc[i].GEOID\n",
    "                    geom_data = gdf[gdf.GEOID == geoId]\n",
    "                    geom_stats = pd.DataFrame(zonal_stats(geom_data, raster_data, affine=affine, stats = ['min','max','median','mean','nodata'], nodata=nodata))\n",
    "                    geom_stats['spatial_unit'] = key\n",
    "                    geom_stats['GEOID'] = geoId\n",
    "                    geom_stats['file'] = '{file}_ST_B10_{basename}_clipped'.format(file = file, basename = basename)\n",
    "                    stats = pd.concat([stats, geom_stats])\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LC08_L2SP_016034_20200922_20201005_02_T1\n",
      "LC08_L2SP_016034_20200906_20200918_02_T1\n",
      "LC08_L2SP_016034_20200720_20210330_02_T1\n",
      "LC08_L2SP_016034_20200704_20200913_02_T1\n",
      "LC08_L2SP_016033_20200922_20201005_02_T1\n",
      "LC08_L2SP_016033_20200906_20200918_02_T1\n",
      "LC08_L2SP_015034_20200713_20200912_02_T1\n",
      "LC08_L2SP_014034_20200722_20200911_02_T1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chasedawson/opt/anaconda3/envs/earth-analytics-python/lib/python3.8/site-packages/rasterstats/main.py:260: UserWarning: Warning: converting a masked element to nan.\n",
      "  feature_stats['nodata'] = float((featmasked == fsrc.nodata).sum())\n"
     ]
    }
   ],
   "source": [
    "cville_stats = compute_zonal_stats(cville_files, {key: cville_shp[key] for key in ['counties', 'tracts', 'blkgps']}, 'cville')\n",
    "easternShore_stats = compute_zonal_stats(easternShore_files, {key: easternShore_shp[key] for key in ['counties', 'tracts', 'blkgps']}, 'easternShore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(stats):\n",
    "    MULTIPLICATIVE_SCALE_FACTOR = 0.00341802\n",
    "    ADDITIVE_SCALE_FACTOR = 149\n",
    "    cols_to_rescale = [\"min\", \"max\", \"mean\", \"median\"]\n",
    "    for col in cols_to_rescale:\n",
    "        stats[col] = stats[col].apply(lambda x: x * MULTIPLICATIVE_SCALE_FACTOR + ADDITIVE_SCALE_FACTOR if type(x) is float else x)\n",
    "        stats[col] = stats[col].apply(lambda x: to_fahrenheit(x) if type(x) is float else x)\n",
    "    return stats\n",
    "\n",
    "def to_fahrenheit(k):\n",
    "    return (k - 273.15) * (9/5) + 32\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "cville_stats = rescale(cville_stats)\n",
    "easternShore_stats = rescale(easternShore_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(stats, metadata, filedata):\n",
    "    entity_to_filename = {x['entityId']: x['filename'] for x in filedata}\n",
    "    print(entity_to_filename)\n",
    "    print([x['entityId'] for x in filedata])\n",
    "    metadata = metadata[['entity_id', 'start_date', 'end_date', 'start_year', 'end_year', 'cloud_cover', 'area', 'shp_percent']]\n",
    "    metadata['file'] = metadata.entity_id.apply(lambda x: entity_to_filename[x].split('.')[0])\n",
    "    stats = stats.join(metadata, on=\"file\")\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cville_stats = merge_data(cville_stats, cville_scene_metadata_f, cville_filedata)\n",
    "easternShore_stats = merge_data(easternShore_stats, easternShore_scene_metadata, easternShore_filedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break into spatial units and save\n",
    "spatial_units = [\"blkgps\", \"tracts\", \"counties\"]\n",
    "for unit in spatial_units:\n",
    "    cville_stats[cville_stats.spatial_unit == unit].to_csv('landsat8_cville_{unit}.csv'.format(unit = unit))\n",
    "    easternShore_stats[easternShore_stats.spatial_unit == unit].to_csv('landsat8_easternShore_{unit}.csv'.format(unit = unit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "# [Downloading Files From Web Using Python](https://www.tutorialspoint.com/downloading-files-from-web-using-python)\n",
    "# [Jupyter Tips and Tricks](https://chrieke.medium.com/jupyter-tips-and-tricks-994fdddb2057)\n",
    "# [USGS/EROS Inventory Service Documentation (Machine-to-Machine) API](https://m2m.cr.usgs.gov/api/docs/json/#section-overview)\n",
    "# [How are files extracted from a tar file using Python?](https://www.tutorialspoint.com/How-are-files-extracted-from-a-tar-file-using-Python)\n",
    "# [How do I use a scale factor with Landsat Level-2 science products?](https://www.usgs.gov/faqs/how-do-i-use-a-scale-factor-landsat-level-2-science-products?qt-news_science_products=0#qt-news_science_products)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earth-analytics-python",
   "language": "python",
   "name": "earth-analytics-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
