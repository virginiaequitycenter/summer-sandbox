{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landsat 8 Surface Temperature Generative Script\n",
    "Chase Dawson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Read in Spatial Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/chasedawson/dev/uva_equity_center/summer-sandbox/landsat8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load libraries\n",
    "import json\n",
    "import requests\n",
    "from dotenv import dotenv_values\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from shapely.geometry import mapping\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import earthpy as et\n",
    "import earthpy.plot as ep\n",
    "from shapely.geometry import Polygon\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# print cwd\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_spatial_data(basename, clip_coast=False):\n",
    "    \"\"\"\n",
    "    Reads in spatial data that is in the format 'basename_spatialUnit.shp'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    basename : str, required\n",
    "        Base name of shape files. For example, if the files you want to read are of the format\n",
    "        'cville_counties.shp', 'cville_blocks.shp', etc. then the basename is 'cville'.\n",
    "        \n",
    "    clip_coast : bool, optional (default is False)\n",
    "        If True, shp data will be compared to coast line data and oceans will be clipped out.\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    dictionary containing geopandas dataframes for each spatial unit\n",
    "    \n",
    "    \"\"\"\n",
    "    print('Reading spatial data for {basename}...'.format(basename = basename))\n",
    "    # read in coast line data \n",
    "    if clip_coast:\n",
    "        ocean = gpd.read_file('water/ne_10m_ocean.shp')\n",
    "    \n",
    "    # store current working directory\n",
    "    og_wd = os.getcwd()\n",
    "    \n",
    "    # change working directory to where spatial data is located\n",
    "    os.chdir(\"../spatial_units/data\")\n",
    "    \n",
    "    # create empty dictionary\n",
    "    data = {}\n",
    "    \n",
    "    if clip_coast:\n",
    "        spatial_units = ['blocks', 'blkgps', 'tracts']\n",
    "\n",
    "        # read in counties and clip first\n",
    "        print(\"Reading counties...\")\n",
    "        counties = gpd.read_file(basename + '_counties.shp')\n",
    "        ocean = ocean.to_crs(counties.crs)\n",
    "        counties = gpd.overlay(counties, ocean, how='difference')\n",
    "        data['counties'] = counties\n",
    "        print(\"Done.\")\n",
    "    \n",
    "        # read in rest of shapefiles, clip respective to counties, store as keys in dict\n",
    "        for spatial_unit in spatial_units:\n",
    "            print(\"Reading {spatial_unit}...\".format(spatial_unit = spatial_unit))\n",
    "            # read in shp file\n",
    "            shp = gpd.read_file(basename + '_{spatial_unit}.shp'.format(spatial_unit = spatial_unit))\n",
    "\n",
    "            # convert coast line data to crs of shp \n",
    "            counties = counties.to_crs(shp.crs)\n",
    "\n",
    "            # clip out ocean\n",
    "            shp = gpd.overlay(shp, counties[['geometry']], how='intersection', keep_geom_type=True)\n",
    "\n",
    "            # add dict with spatial unit as key\n",
    "            data[spatial_unit] = shp\n",
    "            print(\"Done.\")\n",
    "            \n",
    "    else:\n",
    "        spatial_units = ['counties', 'blocks', 'blkgps', 'tracts']\n",
    "        for spatial_unit in spatial_units:\n",
    "            print(\"Reading {spatial_unit}...\".format(spatial_unit = spatial_unit))\n",
    "            shp = gpd.read_file(basename + '_{spatial_unit}.shp'.format(spatial_unit = spatial_unit))\n",
    "            data[spatial_unit] = shp\n",
    "            print(\"Done.\")\n",
    "\n",
    "    # reset back to original working directory\n",
    "    os.chdir(og_wd)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading spatial data for cville...\n",
      "Reading counties...\n"
     ]
    },
    {
     "ename": "DriverError",
     "evalue": "cville_counties.shp: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_err.pyx\u001b[0m in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: cville_counties.shp: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1560cf0cfd74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read in shp files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcville_shp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_spatial_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cville'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0measternShore_shp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_spatial_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eastshore'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_coast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3f6753892deb>\u001b[0m in \u001b[0;36mread_spatial_data\u001b[0;34m(basename, clip_coast)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mspatial_unit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspatial_units\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reading {spatial_unit}...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspatial_unit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial_unit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mshp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_{spatial_unit}.shp'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspatial_unit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial_unit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspatial_unit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/earth-analytics-python/lib/python3.8/site-packages/geopandas/io/file.py\u001b[0m in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfiona_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# In a future Fiona release the crs attribute of features will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/earth-analytics-python/lib/python3.8/site-packages/fiona/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/earth-analytics-python/lib/python3.8/site-packages/fiona/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             c = Collection(path, mode, driver=driver, encoding=encoding,\n\u001b[0m\u001b[1;32m    257\u001b[0m                            layer=layer, enabled_drivers=enabled_drivers, **kwargs)\n\u001b[1;32m    258\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/earth-analytics-python/lib/python3.8/site-packages/fiona/collection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: cville_counties.shp: No such file or directory"
     ]
    }
   ],
   "source": [
    "# read in shp files\n",
    "cville_shp = read_spatial_data('cville')\n",
    "print('\\n')\n",
    "easternShore_shp = read_spatial_data('eastshore', clip_coast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounds(gdf):\n",
    "    \"\"\"\n",
    "    Get lower left and upper right (lat, lng) coordinates for all geometries\n",
    "    in GeoDataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf : GeoDataFrame, required\n",
    "        GeoPandas DataFrame that you want to get a bounding box for.\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    Dictionary containing lower left and upper right (lat, lng) coordinates\n",
    "    of the bounding box. \n",
    "    \n",
    "    \"\"\"\n",
    "    bounds_df = gdf.bounds\n",
    "    lowerLeft = {'latitude': round(bounds_df['miny'].min(), 6), 'longitude': round(bounds_df['minx'].min(), 6)}\n",
    "    upperRight = {'latitude': round(bounds_df['maxy'].max(), 6), 'longitude': round(bounds_df['maxx'].max(), 6)}\n",
    "    return {\n",
    "        'lowerLeft': lowerLeft,\n",
    "        'upperRight': upperRight\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use shpfiles to get bounding boxes for Charlottesville and Eastern Shore\n",
    "cville_bounds = get_bounds(cville_shp['counties'])\n",
    "easternShore_bounds = get_bounds(easternShore_shp['counties'])\n",
    "\n",
    "# get perimeter shps\n",
    "cville_perimeter = cville_shp['counties'].dissolve()\n",
    "easternShore_perimeter = easternShore_shp['counties'].dissolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Landsat 8 Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Methods for working with USGS API ##\n",
    "\n",
    "# API base URL\n",
    "SERVICE_URL = \"https://m2m.cr.usgs.gov/api/api/json/stable/\"\n",
    "\n",
    "def login(username, password):\n",
    "    \"\"\"\n",
    "    Authenticates user given username and password and returns API key.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    username : str, required\n",
    "        USGS account username.\n",
    "        \n",
    "    password : str, required\n",
    "        USGS account password. \n",
    "        \n",
    "    Notes \n",
    "    -----\n",
    "    Go to https://ers.cr.usgs.gov/profile/access to request access \n",
    "    to the API and/or make an account.\n",
    "    \n",
    "    \"\"\"\n",
    "    # login information\n",
    "    payload = {'username': username, 'password': password}\n",
    "\n",
    "    # get apiKey \n",
    "    apiKey = sendRequest(SERVICE_URL + \"login\", payload)\n",
    "    if apiKey == None:\n",
    "        print(\"Login Failed\\n\\n\")\n",
    "    else:\n",
    "        print(\"Login Successful\\n\\n\")\n",
    "    \n",
    "    return apiKey\n",
    "\n",
    "def logout(apiKey):\n",
    "    \"\"\"\n",
    "    Invalidates API key. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    apiKey : str, required\n",
    "        Valid API key. Obtain using the login() method defined above.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Make sure to call when you've finished working to ensure that your \n",
    "    API key can't be used by an unauthorized user.\n",
    "    \n",
    "    \"\"\"\n",
    "    endpoint = \"logout\"\n",
    "    if sendRequest(SERVICE_URL + endpoint, None, apiKey) == None:\n",
    "        print(\"Logged Out\\n\\n\")\n",
    "    else:\n",
    "        print(\"Logout Failed\\n\\n\")\n",
    "\n",
    "def sendRequest(url, data, apiKey = None):\n",
    "    \"\"\"\n",
    "    Sends HTTPS request to specified API endpoint. Main method for interacting\n",
    "    with the API.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str, required\n",
    "        API endpoint you wish you access. Typical format is SERVICE_URL + endpoint, \n",
    "        where endpoint might be something like \"login\" or \"data-search.\" See https://m2m.cr.usgs.gov/api/docs/reference/\n",
    "        for all available endpoints.\n",
    "        \n",
    "    data : dict, required\n",
    "        Request payload. Data required changes based on API endpoint. See \n",
    "        https://m2m.cr.usgs.gov/api/docs/reference/ for input parameters, sample requests,\n",
    "        sample and responses for available endpoints.\n",
    "        \n",
    "    apiKey : str, optional (default is None)\n",
    "        Valid API key. Must be speficied for most requests. \"login\" endpoint doesn't \n",
    "        require an API key since you use that endpoint to retrieve a valid API key.\n",
    "    \n",
    "    \"\"\"\n",
    "    json_data = json.dumps(data)\n",
    "    \n",
    "    if apiKey == None:\n",
    "        response = requests.post(url, json_data)\n",
    "    else:\n",
    "        headers = {'X-Auth-Token': apiKey}\n",
    "        response = requests.post(url, json_data, headers = headers)\n",
    "          \n",
    "    try:\n",
    "        httpStatusCode = response.status_code\n",
    "        \n",
    "        if response == None:\n",
    "            print(\"No output from service!\")\n",
    "            sys.exit()\n",
    "            \n",
    "        output = json.loads(response.text)\n",
    "        if output['errorCode'] != None:\n",
    "            print(output['errorCode'], \"- \", output['errorMessage'])\n",
    "            sys.exit()\n",
    "            \n",
    "        if httpStatusCode == 404:\n",
    "            print(\"404 Not Found\")\n",
    "            sys.exit()\n",
    "            \n",
    "        elif httpStatusCode == 401:\n",
    "            print(\"401 Unauthorized\")\n",
    "            sys.exit()\n",
    "            \n",
    "        elif httpStatusCode == 400:\n",
    "            print(\"Error Code\", httpStatusCode)\n",
    "            sys.exit()\n",
    "            \n",
    "    except Exception as e:\n",
    "        response.close()\n",
    "        print(e)\n",
    "        sys.exit()\n",
    "    \n",
    "    response.close()\n",
    "    return output['data']\n",
    "\n",
    "def getFilename_fromCd(cd):\n",
    "    \"\"\"\n",
    "    Uses content-disposition to infer filename and filetype.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cd : str, required\n",
    "        The Content-Disposition response header from HTTP request \n",
    "        to download a file.\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    Inferred filename and type of provided file : str  \n",
    "    \"\"\"\n",
    "    if not cd:\n",
    "        return None\n",
    "    fname = re.findall('filename=(.+)', cd)\n",
    "    if len(fname) == 0:\n",
    "        return None\n",
    "    \n",
    "    return re.sub('\\\"', '', fname[0]) # remove extra quotes\n",
    "\n",
    "def download_file(url):\n",
    "    \"\"\"\n",
    "    Saves file to local system.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        url: str, required\n",
    "            Link to file to be downloaded.\n",
    "            \n",
    "    Output\n",
    "    ------\n",
    "    Path to downloaded file : str\n",
    "    \"\"\"\n",
    "    res = requests.get(url)\n",
    "    filename = getFilename_fromCd(res.headers.get('content-disposition'))\n",
    "    open(filename, 'wb').write(res.content)\n",
    "    return filename\n",
    "    \n",
    "def search_scenes(apiKey, bounds, start_date, end_date, dataset = \"landsat_ot_c2_l2\", cloud_cover_min = 0, cloud_cover_max = 10):\n",
    "    \"\"\"\n",
    "    Search specified dataset for scenes given spatial and temporal filters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    apiKey : str, required\n",
    "        Valid API key.\n",
    "        \n",
    "    bounds: dict, required\n",
    "        Dictionary with two entries: 'lowerLeft' and 'upperRight' which contain\n",
    "        the lower left and upper right lat, lng coordinates of the bounding box covering\n",
    "        the area of interest.\n",
    "        \n",
    "    start_date: str, required\n",
    "        Format: YYYY-MM-DD\n",
    "        \n",
    "    end_date: str, required\n",
    "        Format: YYYY-MM-DD\n",
    "        \n",
    "    dataset: str, optional (default is 'landsat_ot_c2_l2')\n",
    "        Dataset alias. Use the 'dataset-search' endpoint to discover\n",
    "        which datasets are available.\n",
    "        \n",
    "    cloud_cover_min : int, optional (default is 0)\n",
    "        Minimum cloud coverage percentage. Scenes with cloud coverage less\n",
    "        than this value will not be included in the result.\n",
    "        \n",
    "    cloud_cover_max: int, optional (default is 10)\n",
    "        \n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        'datasetName': dataset,\n",
    "        'startingNumber': 1,\n",
    "        'sceneFilter': {\n",
    "            'spatialFilter': {\n",
    "                'filterType': 'mbr',\n",
    "                'lowerLeft': bounds['lowerLeft'],\n",
    "                'upperRight': bounds['upperRight']\n",
    "            },\n",
    "            'acquisitionFilter': {\n",
    "                'start': start_date,\n",
    "                'end': end_date\n",
    "            },\n",
    "            'cloudCoverFilter': {\n",
    "                'max': 10,\n",
    "                'min': 0,\n",
    "                'includeUnknown': False,\n",
    "            },\n",
    "            'seasonalFilter': [6,7,8,9]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"Searching Scenes...\")\n",
    "    scenes = sendRequest(SERVICE_URL + \"scene-search\", payload, apiKey)\n",
    "    print(\"Found {num_scenes} Scene(s).\".format(num_scenes = scenes['recordsReturned']))\n",
    "    \n",
    "    return scenes\n",
    "\n",
    "def get_acquisitionDates(scenes):\n",
    "    acquisitionDates = {}\n",
    "    for result in scenes['results']:\n",
    "        entityId = result['entityId']\n",
    "        metadata = result['metadata']\n",
    "        for field in metadata:\n",
    "            if field['fieldName'] == \"Acquisition Date\":\n",
    "                date = field['value']\n",
    "                acquisitionDates[entityId] = date\n",
    "                break\n",
    "    return acquisitionDates\n",
    "\n",
    "\n",
    "def get_scene_metadata(scenes):\n",
    "    scene_metadata = []\n",
    "    for result in scenes['results']:\n",
    "        entityId = result['entityId']\n",
    "        acquisitionDate = None\n",
    "        metadata = result['metadata']\n",
    "        for field in metadata:\n",
    "            if field['fieldName'] == \"Acquisition Date\":\n",
    "                acquisitionDate = field['value']\n",
    "                break\n",
    "        cloudCover = result['cloudCover']\n",
    "        publishDate = result['publishDate']\n",
    "        startDate = result['temporalCoverage']['startDate']\n",
    "        endDate = result['temporalCoverage']['endDate']\n",
    "        spatialCoverage = result['spatialCoverage']['coordinates'][0]\n",
    "        spatialBounds = result['spatialBounds']['coordinates'][0]\n",
    "        polygon_geom = Polygon([(x[0], x[1]) for x in spatialCoverage])\n",
    "        new_row = {'entity_id': entityId, 'acquisition_date': acquisitionDate, 'publish_date': publishDate, 'start_date': startDate,\n",
    "                   'end_date': endDate, 'cloud_cover': cloudCover, 'spatial_bounds': spatialBounds, 'spatial_coverage': spatialCoverage,\n",
    "                  'geometry': polygon_geom}        \n",
    "        scene_metadata.append(new_row)\n",
    "    gdf = gpd.GeoDataFrame(scene_metadata, crs=\"EPSG:4326\")\n",
    "    \n",
    "    # convert date-like cols to date cols\n",
    "    date_cols = ['acquisition_date', 'publish_date', 'start_date', 'end_date']\n",
    "    for date_col in date_cols:\n",
    "        gdf[date_col] = gdf[date_col].apply(lambda x: None if x == \"Unknown\" else x)\n",
    "        gdf[date_col] = gdf[date_col].apply(lambda x: datetime.fromisoformat(x.split('.')[0]) if x is not None else x)\n",
    "    \n",
    "    # create year col\n",
    "    gdf['start_year'] = gdf.start_date.apply(lambda x: x.year)\n",
    "    gdf['end_year'] = gdf.end_date.apply(lambda x: x.year)\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "def get_area(shp):\n",
    "    # get area of shp in km^2\n",
    "    return round(shp.geometry.to_crs(\"EPSG:3395\").map(lambda p: p.area / 10**6).iloc[0], 6)\n",
    "        \n",
    "def intersection_stats(shp, scene):\n",
    "    intersection = gpd.overlay(shp, scene, how='intersection')\n",
    "    if len(intersection) == 0:\n",
    "        return {'area': 0, 'shp_percent': 0}\n",
    "    \n",
    "    # get area of intersection in km^2\n",
    "    intersect_area = get_area(intersection)\n",
    "    \n",
    "    # get area of original shp in km^2\n",
    "    shp_area = get_area(shp)\n",
    "    \n",
    "    # compute percentage of intersection of shp\n",
    "    percentage = (intersect_area / shp_area) * 100\n",
    "    \n",
    "    return {'area': intersect_area, 'shp_percent': percentage}\n",
    "    \n",
    "\n",
    "def filter_scenes(scenes, shp_data):\n",
    "    # total number of counties that could be captured by \n",
    "    total_geoms = len(shp_data)\n",
    "    \n",
    "        \n",
    "def get_sceneIds(scenes):\n",
    "    \"\"\"\n",
    "    Parses scene data to return list of scene ids.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    scenes : object, required\n",
    "        Output from search_scenes().\n",
    "        \n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    scene ids : list\n",
    "    \n",
    "    \"\"\"\n",
    "    sceneIds = []\n",
    "    for result in scenes['results']:\n",
    "        sceneIds.append(result['entityId'])\n",
    "    return sceneIds\n",
    "\n",
    "def download_scenes(apiKey, sceneIds, label, dataset = \"landsat_ot_c2_l2\"):\n",
    "    \"\"\"\n",
    "    Downloads scenes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    apiKey : str, required\n",
    "        Valid API key.\n",
    "        \n",
    "    scenes : object, required\n",
    "        Scenes you wish to download. Returned from search_scenes().\n",
    "        \n",
    "    label : str, required\n",
    "        Label for your download request.\n",
    "        \n",
    "    dataset : str, optional (default is 'landsat_ot_c2_l2')\n",
    "        Must be the dataset the scenes are from. \n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    Paths to downloaded files : list\n",
    "    \"\"\"\n",
    "        \n",
    "    # download options\n",
    "    payload = {\n",
    "        'datasetName': dataset,\n",
    "        'entityIds': sceneIds,\n",
    "    }\n",
    "    \n",
    "    downloadOptions = sendRequest(SERVICE_URL + \"download-options\", payload, apiKey)\n",
    "    \n",
    "    # aggregate list of available products\n",
    "    downloads = []\n",
    "    for product in downloadOptions:\n",
    "        # make sure the product is available for this scene\n",
    "        if product['available'] == True:\n",
    "            downloads.append({'entityId': product['entityId'],\n",
    "                             'productId': product['id']})\n",
    "            \n",
    "    if downloads:\n",
    "        requestedDownloadsCount = len(downloads)\n",
    "        print(\"Number of Requested Downloads: {requestedDownloadsCount}\".format(requestedDownloadsCount = requestedDownloadsCount))\n",
    "        print(\"Downloading Now...\")\n",
    "        payload = {\n",
    "            'downloads': downloads,\n",
    "            'label': label\n",
    "        }\n",
    "        requestResults = sendRequest(SERVICE_URL + \"download-request\", payload, apiKey)\n",
    "        if requestResults['preparingDownloads'] != None and len(requestResults['preparingDownloads']) > 0:\n",
    "            payload = {'label': label}\n",
    "            downloadUrls = sendRequest(SERVICE_URL + \"download-retrieve\", payload, apiKey)\n",
    "            downloadIds = []\n",
    "            for download in downloadUrls['available']:\n",
    "                downloadIds.append(download['downloadId'])\n",
    "                \n",
    "            for download in downloadUrls['requested']:\n",
    "                downloadIds.append(download['downloadId'])\n",
    "                \n",
    "            while len(downloadIds) < requestedDownloadsCount:\n",
    "                preparingDownloads = requestedDownloadsCount - len(downloadIds)\n",
    "                print('\\n', preparingDownloads, \"download(s) are not yet available. Waiting for 30 seconds.\\n\")\n",
    "                time.sleep(30)\n",
    "                print(\"Trying to retrieve data.\\n\")\n",
    "                downloadUrls = sendRequest(SERVICE_URL + \"download-retrieve\", payload, apiKey)\n",
    "                for download in downloadUrls['available']:\n",
    "                    if download['downloadId'] not in downloadIds:\n",
    "                        downloadIds.append(download['downloadId'])\n",
    "        else:\n",
    "            # get all available downloads\n",
    "            # search requested downloads to get metadata\n",
    "            payload = {\n",
    "                'label': label\n",
    "            }\n",
    "            download_search = sendRequest(SERVICE_URL + \"download-search\", payload, apiKey)\n",
    "            download_metadata = {}\n",
    "            for search_result in download_search:\n",
    "                entityId = search_result['entityId']\n",
    "                displayId = search_result['displayId']\n",
    "                downloadId = search_result['downloadId']\n",
    "                download_metadata[downloadId] = {'entityId': entityId, 'displayId': displayId}\n",
    "                \n",
    "            files = []\n",
    "            for download in requestResults['availableDownloads']:\n",
    "                url = download['url']\n",
    "                downloadId = download['downloadId']\n",
    "                filename = download_file(url)\n",
    "                print(\"{filename} Downloaded Successfully.\".format(filename = filename))\n",
    "                filedata = {\n",
    "                    'entityId': download_metadata[downloadId]['entityId'],\n",
    "                    'displayId': download_metadata[downloadId]['displayId'],\n",
    "                    'filename': filename\n",
    "                }\n",
    "                files.append(filedata)\n",
    "                \n",
    "            print(\"\\nAll files have been downloaded.\\n\")\n",
    "            return files\n",
    "        \n",
    "    else:\n",
    "        print(\"No available products.\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Successful\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# log in to retrieve API key\n",
    "config = dotenv_values('.env')\n",
    "apiKey = login(config['USGS_USERNAME'], config['USGS_PASSWORD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"landsat_ot_c2_l2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define temporal filter start and end constants\n",
    "SEARCH_START = \"2016-06-00\"\n",
    "SEARCH_END = \"2020-09-30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersection_stats(shp, metadata):\n",
    "    area = []\n",
    "    shp_percent = []\n",
    "    for i in range(len(metadata)):\n",
    "        stats = intersection_stats(shp, metadata.iloc[[i]])\n",
    "        area.append(stats['area'])\n",
    "        shp_percent.append(stats['shp_percent'])\n",
    "    metadata['area'] = area\n",
    "    metadata['shp_percent'] = shp_percent\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_scenes(metadata, shp):\n",
    "    # filter scenes with 2 constraints:\n",
    "    # 1. each year is represented (or number of years represented is maximized)\n",
    "    # 2. as much of shp is covered as possible\n",
    "    \n",
    "    to_keep = [] # list of entity ids of scenes to keep \n",
    "    \n",
    "    years = metadata.start_year.value_counts().index.tolist()\n",
    "    years.sort()\n",
    "    # iterate over each year \n",
    "    for year in years:\n",
    "        metadata_yr = metadata[metadata.start_year == year]\n",
    "        \n",
    "        # sort by shp_percentage\n",
    "        metadata_yr = metadata_yr.sort_values(by=[\"shp_percent\", \"cloud_cover\"], ascending=(False, True))\n",
    "        \n",
    "        shp_left = shp # keep track of what part of shp hasn't been covered yet\n",
    "        \n",
    "        # iterate over scenes in year, starting with the scene that has the largest intersection with shp\n",
    "        for i in range(len(metadata_yr)):\n",
    "            scene = metadata_yr.iloc[[i]].to_crs(shp.crs) # load scene and convert to crs of shp\n",
    "            intersection = gpd.overlay(shp_left, scene, how='intersection')\n",
    "            stats = intersection_stats(shp_left, scene)\n",
    "            intersected = len(intersection) > 0\n",
    "            if stats['shp_percent'] == 100.0:\n",
    "                # add scene to keep \n",
    "                to_keep.append(scene.iloc[0]['entity_id'])\n",
    "                break\n",
    "                \n",
    "            elif intersected:\n",
    "                # add scene to keep\n",
    "                to_keep.append(scene.iloc[0]['entity_id'])\n",
    "                \n",
    "                # update shp_left\n",
    "                shp_left = intersection\n",
    "                    \n",
    "    # filter metadata to entity ids in to_keep and return\n",
    "    return metadata[metadata.entity_id.isin(to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Scenes...\n",
      "Found 49 Scene(s).\n",
      "Filtered down to 27 Scene(s).\n"
     ]
    }
   ],
   "source": [
    "# get data for Charlottesville\n",
    "cville_scenes = search_scenes(apiKey, cville_bounds, SEARCH_START, SEARCH_END)\n",
    "cville_scene_metadata = get_scene_metadata(cville_scenes)\n",
    "\n",
    "# convert metadata to crs of cville spatial data\n",
    "cville_scene_metadata = cville_scene_metadata.to_crs(cville_shp['counties'].crs)\n",
    "cville_scene_metadata = get_intersection_stats(cville_shp['counties'].dissolve(), cville_scene_metadata)\n",
    "\n",
    "# filter\n",
    "cville_scene_metadata_f = filter_scenes(cville_scene_metadata, cville_shp['counties'].dissolve())\n",
    "print(\"Filtered down to {num_scenes} Scene(s).\".format(num_scenes = len(cville_scene_metadata_f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Scenes...\n",
      "Found 58 Scene(s).\n",
      "Filtered down to 5 Scene(s).\n"
     ]
    }
   ],
   "source": [
    "# search and filter scenes for the Eastern Shore\n",
    "easternShore_scenes = search_scenes(apiKey, easternShore_bounds, SEARCH_START, SEARCH_END)\n",
    "easternShore_scene_metadata = get_scene_metadata(easternShore_scenes)\n",
    "\n",
    "# convert metadata to crs of eastern shore spatial data\n",
    "easternShore_scene_metadata = easternShore_scene_metadata.to_crs(easternShore_shp['counties'].crs)\n",
    "easternShore_scene_metadata = get_intersection_stats(easternShore_shp['counties'].dissolve(), easternShore_scene_metadata)\n",
    "\n",
    "# filter\n",
    "easternShore_scene_metadata_f = filter_scenes(easternShore_scene_metadata, easternShore_shp['counties'].dissolve())\n",
    "print(\"Filtered down to {num_scenes} Scene(s).\".format(num_scenes = len(easternShore_scene_metadata_f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Requested Downloads: 7\n",
      "Downloading Now...\n",
      "LC08_L2SP_016034_20200922_20201005_02_T1.tar Downloaded Successfully.\n",
      "LC08_L2SP_016034_20200906_20200918_02_T1.tar Downloaded Successfully.\n",
      "LC08_L2SP_016034_20200720_20210330_02_T1.tar Downloaded Successfully.\n",
      "LC08_L2SP_016034_20200704_20200913_02_T1.tar Downloaded Successfully.\n",
      "LC08_L2SP_016033_20200922_20201005_02_T1.tar Downloaded Successfully.\n",
      "LC08_L2SP_016033_20200906_20200918_02_T1.tar Downloaded Successfully.\n",
      "LC08_L2SP_015034_20200713_20200912_02_T1.tar Downloaded Successfully.\n",
      "\n",
      "All files have been downloaded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download cville data (just for 2020)\n",
    "cville_filedata = download_scenes(\n",
    "    apiKey,\n",
    "    cville_scene_metadata_f[cville_scene_metadata_f.start_year == 2020].entity_id.tolist(),\n",
    "    'cville_summer_2020'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LC08_L2SP_016034_20200922_20201005_02_T1.tar',\n",
       " 'LC08_L2SP_016034_20200906_20200918_02_T1.tar',\n",
       " 'LC08_L2SP_016034_20200720_20210330_02_T1.tar',\n",
       " 'LC08_L2SP_016034_20200704_20200913_02_T1.tar',\n",
       " 'LC08_L2SP_016033_20200922_20201005_02_T1.tar',\n",
       " 'LC08_L2SP_016033_20200906_20200918_02_T1.tar',\n",
       " 'LC08_L2SP_015034_20200713_20200912_02_T1.tar']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cville_files = [x['filename'] for x in cville_filedata]\n",
    "cville_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Requested Downloads: 1\n",
      "Downloading Now...\n",
      "LC08_L2SP_014034_20200722_20200911_02_T1.tar Downloaded Successfully.\n",
      "\n",
      "All files have been downloaded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "easternShore_filedata = download_scenes(\n",
    "    apiKey, \n",
    "    easternShore_scene_metadata_f[easternShore_scene_metadata_f.start_year == 2020].entity_id.tolist(),\n",
    "    'easternShore_summer_2020'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LC08_L2SP_014034_20200722_20200911_02_T1.tar']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easternShore_files = [x['filename'] for x in easternShore_filedata]\n",
    "easternShore_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged Out\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logout of API\n",
    "# if successful, apiKey is now invalid\n",
    "logout(apiKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/chasedawson/dev/uva_equity_center/climate_equity'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make directories where data will be extracted\n",
    "os.mkdir(\"landsat8_c2_l2_data\")\n",
    "os.chdir(\"landsat8_c2_l2_data\")\n",
    "os.mkdir(\"cville\")\n",
    "os.mkdir(\"easternShore\")\n",
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "def extract_tar(tar, extract_to):\n",
    "    \"\"\"\n",
    "    Extracts tar file to specified location.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tar : str, required\n",
    "        Path to tar file that will be extracted.\n",
    "        \n",
    "    extract_to : str, required\n",
    "        Path to folder in which the tar file will be extracted.\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\"Extracting\", tar, \"...\")\n",
    "    my_tar = tarfile.open(tar)\n",
    "    my_tar.extractall(extract_to)\n",
    "    my_tar.close()\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths for extracted tar files for both geographic regions\n",
    "# of interest\n",
    "cville_tar_path = \"./landsat8_c2_l2_data/cville\"\n",
    "easternShore_tar_path = \"./landsat8_c2_l2_data/easternShore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting LC08_L2SP_016034_20200922_20201005_02_T1.tar ...\n",
      "Done.\n",
      "Extracting LC08_L2SP_016034_20200906_20200918_02_T1.tar ...\n",
      "Done.\n",
      "Extracting LC08_L2SP_016034_20200720_20210330_02_T1.tar ...\n",
      "Done.\n",
      "Extracting LC08_L2SP_016034_20200704_20200913_02_T1.tar ...\n",
      "Done.\n",
      "Extracting LC08_L2SP_016033_20200922_20201005_02_T1.tar ...\n",
      "Done.\n",
      "Extracting LC08_L2SP_016033_20200906_20200918_02_T1.tar ...\n",
      "Done.\n",
      "Extracting LC08_L2SP_015034_20200713_20200912_02_T1.tar ...\n",
      "Done.\n",
      "Extracting LC08_L2SP_014034_20200722_20200911_02_T1.tar ...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# extract cville files and easternShore files\n",
    "for tar in cville_files:\n",
    "    extract_tar(tar, cville_tar_path)\n",
    "    \n",
    "for tar in easternShore_files:\n",
    "    extract_tar(tar, easternShore_tar_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete original tar files\n",
    "for tar in cville_files:\n",
    "    os.remove(tar)\n",
    "    \n",
    "for tar in easternShore_files:\n",
    "    os.remove(tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tar_endings(files):\n",
    "    \"\"\"\n",
    "    Removes .tar endings from a list of filenames.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list, required\n",
    "        List of tar filenames.\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    list of filenames with .tar endings removed : list\n",
    "    \n",
    "    \"\"\"\n",
    "    reformatted_names = []\n",
    "    for file in files:\n",
    "        reformatted_names.append(file.split('.')[0])\n",
    "    return reformatted_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove .tar endings\n",
    "cville_files = remove_tar_endings(cville_files)\n",
    "easternShore_files = remove_tar_endings(easternShore_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip Raster Data with Shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename ending for raster temperature data\n",
    "ST_ENDING = \"_ST_B10.TIF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_crs(shp_data, raster_src):\n",
    "    for key in shp_data:\n",
    "        shp_data[key] = shp_data[key].to_crs(raster_src.crs)\n",
    "    return shp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make clipped dirs in both cville and easternShore\n",
    "os.mkdir(\"landsat8_c2_l2_data/cville/clipped\")\n",
    "os.mkdir(\"landsat8_c2_l2_data/easternShore/clipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_rasters(files, shp_data, basename):\n",
    "    \"\"\"\n",
    "    Clip set of rasters given shapefiles. Saves these files in\n",
    "    'landsat8_c2_l2_data/basename/clipped'. With the geographic region and\n",
    "    spatial resolution encoded in the name.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list, required\n",
    "        List of raster file base names to be clipped.\n",
    "    \n",
    "    shp_data : dict, required\n",
    "        Geometry data for given region. A dictionary containing geometries for different spatial resolutions.\n",
    "        \n",
    "    basename : str, required\n",
    "        Name of geographic region. Either 'cville' or 'easternShore'.\n",
    "    \"\"\"\n",
    "    full_boundary = shp_data['counties'].dissolve()\n",
    "    for file in files:\n",
    "        path = 'landsat8_c2_l2_data/{basename}/{file}{ending}'.format(basename = basename, file = file, ending = ST_ENDING)\n",
    "        with rio.open(path) as src:\n",
    "            full_boundary = full_boundary.to_crs(src.crs)\n",
    "        raster_data = rxr.open_rasterio(path, masked=True).squeeze()\n",
    "        raster_clipped = raster_data.rio.clip(full_boundary.geometry.apply(mapping), full_boundary.crs)\n",
    "        # export clipped raster\n",
    "        new_filename = \"{file}_ST_B10_{basename}_clipped.TIF\".format(basename = basename, file = file)\n",
    "        raster_clipped.rio.to_raster(os.path.join(\"landsat8_c2_l2_data\", basename, \"clipped\", new_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip and save rasters for cville and eastern shore\n",
    "clip_rasters(cville_files, cville_shp, \"cville\")\n",
    "clip_rasters(easternShore_files, easternShore_shp, \"easternShore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from rasterio.plot import plotting_extent\n",
    "\n",
    "def plot_clipped_raster(file, shp, basename):\n",
    "    path = 'landsat8_c2_l2_data/{basename}/clipped/{file}_ST_B10_{basename}_clipped.TIF'.format(basename = basename, file = file)\n",
    "    with rio.open(path) as src:\n",
    "        st_data = src.read()\n",
    "        nodata = src.nodata\n",
    "        \n",
    "        # change type of st_data to float64 to include nan\n",
    "        st_data = st_data.astype('float64')\n",
    "        st_data[st_data == nodata] = np.nan\n",
    "        \n",
    "        shp = shp.to_crs(src.crs)\n",
    "        f, ax = plt.subplots()\n",
    "        # ax.imshow(st_data[0], cm.inferno)\n",
    "        ep.plot_bands(st_data, ax=ax, extent=plotting_extent(src), cmap=cm.inferno)\n",
    "        shp.boundary.plot(ax=ax)\n",
    "        plt.show()\n",
    "\n",
    "# plot clipped rasters and shp files\n",
    "def plot_clipped_rasters(files, shp_data, basename):\n",
    "    for file in files:\n",
    "        path = 'landsat8_c2_l2_data/{basename}/clipped/{file}_ST_B10_{basename}_clipped.TIF'.format(basename = basename, file = file)\n",
    "        with rio.open(path) as src:\n",
    "            st_data = src.read()\n",
    "            nodata = src.nodata\n",
    "            \n",
    "            # change type of st_data to float64 to include nan\n",
    "            st_data = st_data.astype('float64')\n",
    "            st_data[st_data == nodata] = np.nan\n",
    "            \n",
    "            for key in shp_data:\n",
    "                shp = shp_data[key]\n",
    "                shp = shp.to_crs(src.crs)\n",
    "                f, ax = plt.subplots()\n",
    "                ax.imshow(st_data[0], cm.inferno)\n",
    "                shp.boundary.plot(ax=ax)\n",
    "                ax.set_title('{basename} {key}'.format(basename = basename, key = key))\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zonal Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterstats import zonal_stats\n",
    "import pandas as pd\n",
    "\n",
    "def my_masked_count(x):\n",
    "    return np.ma.count_masked(x)\n",
    "\n",
    "# read in clipped files\n",
    "# stats notes\n",
    "# count is the non_masked count\n",
    "# masked values can be nodata or otherwise masked\n",
    "def compute_zonal_stats(files, shp_data, basename):\n",
    "    stats = pd.DataFrame()\n",
    "    for file in files:\n",
    "        print(file)\n",
    "        path = 'landsat8_c2_l2_data/{basename}/clipped/{file}_ST_B10_{basename}_clipped.TIF'.format(basename = basename, file = file)\n",
    "        with rio.open(path) as src:\n",
    "            affine = src.transform\n",
    "            nodata = src.profile['nodata']\n",
    "            raster_data = src.read(1)\n",
    "            for key in shp_data:\n",
    "                gdf = shp_data[key]\n",
    "                gdf = gdf.to_crs(src.crs)\n",
    "                # plot_clipped_raster(file, gdf, basename)\n",
    "                for i in range(len(gdf)):\n",
    "                    geoId = gdf.iloc[i].GEOID\n",
    "                    geom_data = gdf[gdf.GEOID == geoId]\n",
    "                    geom_stats = pd.DataFrame(zonal_stats(geom_data, raster_data, affine=affine, stats = ['min','max','median','mean','nodata','count'], add_stats={'masked_count': my_masked_count}, nodata=nodata))\n",
    "                    geom_stats['spatial_unit'] = key\n",
    "                    geom_stats['GEOID'] = geoId\n",
    "                    geom_stats['file'] = file\n",
    "                    geom_stats['basename'] = basename\n",
    "                    geom_stats['full_path'] = path\n",
    "                    stats = pd.concat([stats, geom_stats])\n",
    "    # compute nodata_prop\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>acquisition_date</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>cloud_cover</th>\n",
       "      <th>spatial_bounds</th>\n",
       "      <th>spatial_coverage</th>\n",
       "      <th>geometry</th>\n",
       "      <th>start_year</th>\n",
       "      <th>end_year</th>\n",
       "      <th>area</th>\n",
       "      <th>shp_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LC80160332020266LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-12-02 07:54:10</td>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[[-79.73837, 37.83651], [-79.73837, 39.95744],...</td>\n",
       "      <td>[[-79.73837, 38.23916], [-77.62342, 37.83651],...</td>\n",
       "      <td>POLYGON ((-79.73837 38.23916, -77.62342 37.836...</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>4129.867083</td>\n",
       "      <td>45.819210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LC80160342020266LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-12-02 14:35:32</td>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[[-80.14489, 36.40826], [-80.14489, 38.52658],...</td>\n",
       "      <td>[[-80.14489, 36.80681], [-78.06929, 36.40826],...</td>\n",
       "      <td>POLYGON ((-80.14489 36.80681, -78.06929 36.408...</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>8756.939748</td>\n",
       "      <td>97.154715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LC80160332020250LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-12-02 06:19:09</td>\n",
       "      <td>2020-09-06</td>\n",
       "      <td>2020-09-06</td>\n",
       "      <td>0.44</td>\n",
       "      <td>[[-79.73071, 37.83621], [-79.73071, 39.95716],...</td>\n",
       "      <td>[[-79.73071, 38.23894], [-77.6158, 37.83621], ...</td>\n",
       "      <td>POLYGON ((-79.73071 38.23894, -77.61580 37.836...</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>4110.276417</td>\n",
       "      <td>45.601859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LC80160342020250LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-12-02 06:24:01</td>\n",
       "      <td>2020-09-06</td>\n",
       "      <td>2020-09-06</td>\n",
       "      <td>0.09</td>\n",
       "      <td>[[-80.13726, 36.40795], [-80.13726, 38.52635],...</td>\n",
       "      <td>[[-80.13726, 36.80658], [-78.06168, 36.40795],...</td>\n",
       "      <td>POLYGON ((-80.13726 36.80658, -78.06168 36.407...</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>8761.611621</td>\n",
       "      <td>97.206547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LC80160342020202LGN01</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-03-30 10:44:09</td>\n",
       "      <td>2020-07-20</td>\n",
       "      <td>2020-07-20</td>\n",
       "      <td>9.89</td>\n",
       "      <td>[[-80.13223, 36.40817], [-80.13223, 38.52677],...</td>\n",
       "      <td>[[-80.13223, 36.80681], [-78.05767, 36.40817],...</td>\n",
       "      <td>POLYGON ((-80.13223 36.80681, -78.05767 36.408...</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>8766.755681</td>\n",
       "      <td>97.263619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LC80150342020195LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-12-01 08:49:22</td>\n",
       "      <td>2020-07-13</td>\n",
       "      <td>2020-07-13</td>\n",
       "      <td>8.25</td>\n",
       "      <td>[[-78.59079, 36.40771], [-78.59079, 38.52642],...</td>\n",
       "      <td>[[-78.59079, 36.80651], [-76.51531, 36.40771],...</td>\n",
       "      <td>POLYGON ((-78.59079 36.80651, -76.51531 36.407...</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>2745.316903</td>\n",
       "      <td>30.458184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LC80160342020186LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-12-01 12:10:10</td>\n",
       "      <td>2020-07-04</td>\n",
       "      <td>2020-07-04</td>\n",
       "      <td>5.29</td>\n",
       "      <td>[[-80.13743, 36.40785], [-80.13743, 38.52648],...</td>\n",
       "      <td>[[-80.13743, 36.80654], [-78.0627, 36.40785], ...</td>\n",
       "      <td>POLYGON ((-80.13743 36.80654, -78.06270 36.407...</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>8761.698846</td>\n",
       "      <td>97.207515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LC80150342019272LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-09-29</td>\n",
       "      <td>2019-09-29</td>\n",
       "      <td>0.92</td>\n",
       "      <td>[[-78.59598, 36.40802], [-78.59598, 38.52639],...</td>\n",
       "      <td>[[-78.59598, 36.80669], [-76.51954, 36.40802],...</td>\n",
       "      <td>POLYGON ((-78.59598 36.80669, -76.51954 36.408...</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>2781.158033</td>\n",
       "      <td>30.855827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LC80160332019263LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-09-20</td>\n",
       "      <td>2019-09-20</td>\n",
       "      <td>0.53</td>\n",
       "      <td>[[-79.72973, 37.83664], [-79.72973, 39.95751],...</td>\n",
       "      <td>[[-79.72973, 38.23923], [-77.61483, 37.83664],...</td>\n",
       "      <td>POLYGON ((-79.72973 38.23923, -77.61483 37.836...</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>4100.785398</td>\n",
       "      <td>45.496560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LC80160342019263LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-09-20</td>\n",
       "      <td>2019-09-20</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[[-80.13634, 36.40788], [-80.13634, 38.52616],...</td>\n",
       "      <td>[[-80.13634, 36.80638], [-78.06078, 36.40788],...</td>\n",
       "      <td>POLYGON ((-80.13634 36.80638, -78.06078 36.407...</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>8761.744917</td>\n",
       "      <td>97.208026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LC80150342019224LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-08-12</td>\n",
       "      <td>2019-08-12</td>\n",
       "      <td>6.23</td>\n",
       "      <td>[[-78.58301, 36.40793], [-78.58301, 38.52654],...</td>\n",
       "      <td>[[-78.58301, 36.80669], [-76.50726, 36.40793],...</td>\n",
       "      <td>POLYGON ((-78.58301 36.80669, -76.50726 36.407...</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>2695.454907</td>\n",
       "      <td>29.904985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LC80150342019208LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-07-27</td>\n",
       "      <td>2019-07-27</td>\n",
       "      <td>3.60</td>\n",
       "      <td>[[-78.58056, 36.40779], [-78.58056, 38.52644],...</td>\n",
       "      <td>[[-78.58056, 36.80655], [-76.50493, 36.40779],...</td>\n",
       "      <td>POLYGON ((-78.58056 36.80655, -76.50493 36.407...</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>2679.130377</td>\n",
       "      <td>29.723871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LC80160332019183LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-07-02</td>\n",
       "      <td>2019-07-02</td>\n",
       "      <td>8.99</td>\n",
       "      <td>[[-79.7307, 37.83649], [-79.7307, 39.95762], [...</td>\n",
       "      <td>[[-79.7307, 38.23928], [-77.61643, 37.83649], ...</td>\n",
       "      <td>POLYGON ((-79.73070 38.23928, -77.61643 37.836...</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>4106.674987</td>\n",
       "      <td>45.561903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LC80160342019183LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-07-02</td>\n",
       "      <td>2019-07-02</td>\n",
       "      <td>3.64</td>\n",
       "      <td>[[-80.13751, 36.40787], [-80.13751, 38.52649],...</td>\n",
       "      <td>[[-80.13751, 36.80656], [-78.06257, 36.40787],...</td>\n",
       "      <td>POLYGON ((-80.13751 36.80656, -78.06257 36.407...</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>8761.769410</td>\n",
       "      <td>97.208298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LC80150342018269LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-09-26</td>\n",
       "      <td>2018-09-26</td>\n",
       "      <td>7.88</td>\n",
       "      <td>[[-78.58312, 36.40782], [-78.58312, 38.52634],...</td>\n",
       "      <td>[[-78.58312, 36.8066], [-76.50702, 36.40782], ...</td>\n",
       "      <td>POLYGON ((-78.58312 36.80660, -76.50702 36.407...</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>2696.040403</td>\n",
       "      <td>29.911481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LC80150342018189LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-07-08</td>\n",
       "      <td>2018-07-08</td>\n",
       "      <td>0.76</td>\n",
       "      <td>[[-78.5802, 36.40798], [-78.5802, 38.52675], [...</td>\n",
       "      <td>[[-78.5802, 36.80708], [-76.50398, 36.40798], ...</td>\n",
       "      <td>POLYGON ((-78.58020 36.80708, -76.50398 36.407...</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>2676.366205</td>\n",
       "      <td>29.693204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LC80160332018180LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-06-29</td>\n",
       "      <td>2018-06-29</td>\n",
       "      <td>4.54</td>\n",
       "      <td>[[-79.71923, 37.8365], [-79.71923, 39.95777], ...</td>\n",
       "      <td>[[-79.71923, 38.23955], [-77.60454, 37.8365], ...</td>\n",
       "      <td>POLYGON ((-79.71923 38.23955, -77.60454 37.836...</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>4067.971111</td>\n",
       "      <td>45.132499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>LC80160342017273LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-09-30</td>\n",
       "      <td>2017-09-30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>[[-80.15024, 36.40789], [-80.15024, 38.52615],...</td>\n",
       "      <td>[[-80.15024, 36.80627], [-78.07497, 36.40789],...</td>\n",
       "      <td>POLYGON ((-80.15024 36.80627, -78.07497 36.407...</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>8751.272784</td>\n",
       "      <td>97.091842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>LC80150342017266LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-09-23</td>\n",
       "      <td>2017-09-23</td>\n",
       "      <td>2.55</td>\n",
       "      <td>[[-78.5961, 36.40789], [-78.5961, 38.52631], [...</td>\n",
       "      <td>[[-78.5961, 36.80646], [-76.51989, 36.40789], ...</td>\n",
       "      <td>POLYGON ((-78.59610 36.80646, -76.51989 36.407...</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>2781.823351</td>\n",
       "      <td>30.863208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>LC80150342017250LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-09-07</td>\n",
       "      <td>2017-09-07</td>\n",
       "      <td>1.32</td>\n",
       "      <td>[[-78.58755, 36.4079], [-78.58755, 38.52638], ...</td>\n",
       "      <td>[[-78.58755, 36.80659], [-76.51129, 36.4079], ...</td>\n",
       "      <td>POLYGON ((-78.58755 36.80659, -76.51129 36.407...</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>2725.789934</td>\n",
       "      <td>30.241540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LC80150342017234LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>7.14</td>\n",
       "      <td>[[-78.59362, 36.40791], [-78.59362, 38.52644],...</td>\n",
       "      <td>[[-78.59362, 36.80665], [-76.51741, 36.40791],...</td>\n",
       "      <td>POLYGON ((-78.59362 36.80665, -76.51741 36.407...</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>2765.050416</td>\n",
       "      <td>30.677119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>LC80160332017225LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-08-13</td>\n",
       "      <td>2017-08-13</td>\n",
       "      <td>9.57</td>\n",
       "      <td>[[-79.73211, 37.83657], [-79.73211, 39.95754],...</td>\n",
       "      <td>[[-79.73211, 38.23922], [-77.61757, 37.83657],...</td>\n",
       "      <td>POLYGON ((-79.73211 38.23922, -77.61757 37.836...</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>4109.944407</td>\n",
       "      <td>45.598176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>LC80160342017161LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-06-10</td>\n",
       "      <td>2017-06-10</td>\n",
       "      <td>0.44</td>\n",
       "      <td>[[-80.14113, 36.40817], [-80.14113, 38.52672],...</td>\n",
       "      <td>[[-80.14113, 36.80685], [-78.06613, 36.40817],...</td>\n",
       "      <td>POLYGON ((-80.14113 36.80685, -78.06613 36.408...</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>8760.048674</td>\n",
       "      <td>97.189207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>LC80150342017154LGN00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-06-03</td>\n",
       "      <td>2017-06-03</td>\n",
       "      <td>5.56</td>\n",
       "      <td>[[-78.59533, 36.40806], [-78.59533, 38.52674],...</td>\n",
       "      <td>[[-78.59533, 36.80695], [-76.51932, 36.40806],...</td>\n",
       "      <td>POLYGON ((-78.59533 36.80695, -76.51932 36.408...</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>2775.803655</td>\n",
       "      <td>30.796422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>LC80160342016239LGN01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2016-08-26</td>\n",
       "      <td>2016-08-26</td>\n",
       "      <td>4.82</td>\n",
       "      <td>[[-80.13243, 36.40816], [-80.13243, 38.52659],...</td>\n",
       "      <td>[[-80.13243, 36.80668], [-78.05731, 36.40816],...</td>\n",
       "      <td>POLYGON ((-80.13243 36.80668, -78.05731 36.408...</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>8766.156076</td>\n",
       "      <td>97.256966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>LC80150342016200LGN01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2016-07-18</td>\n",
       "      <td>2016-07-18</td>\n",
       "      <td>4.73</td>\n",
       "      <td>[[-78.59052, 36.40786], [-78.59052, 38.52652],...</td>\n",
       "      <td>[[-78.59052, 36.80668], [-76.51465, 36.40786],...</td>\n",
       "      <td>POLYGON ((-78.59052 36.80668, -76.51465 36.407...</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>2744.084352</td>\n",
       "      <td>30.444509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>LC80150342016184LGN01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2016-07-02</td>\n",
       "      <td>2016-07-02</td>\n",
       "      <td>6.01</td>\n",
       "      <td>[[-78.58765, 36.40813], [-78.58765, 38.5268], ...</td>\n",
       "      <td>[[-78.58765, 36.80702], [-76.51162, 36.40813],...</td>\n",
       "      <td>POLYGON ((-78.58765 36.80702, -76.51162 36.408...</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>2725.831395</td>\n",
       "      <td>30.242000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                entity_id acquisition_date        publish_date start_date  \\\n",
       "2   LC80160332020266LGN00             None 2020-12-02 07:54:10 2020-09-22   \n",
       "3   LC80160342020266LGN00             None 2020-12-02 14:35:32 2020-09-22   \n",
       "4   LC80160332020250LGN00             None 2020-12-02 06:19:09 2020-09-06   \n",
       "5   LC80160342020250LGN00             None 2020-12-02 06:24:01 2020-09-06   \n",
       "7   LC80160342020202LGN01             None 2021-03-30 10:44:09 2020-07-20   \n",
       "8   LC80150342020195LGN00             None 2020-12-01 08:49:22 2020-07-13   \n",
       "9   LC80160342020186LGN00             None 2020-12-01 12:10:10 2020-07-04   \n",
       "11  LC80150342019272LGN00             None                 NaT 2019-09-29   \n",
       "12  LC80160332019263LGN00             None                 NaT 2019-09-20   \n",
       "13  LC80160342019263LGN00             None                 NaT 2019-09-20   \n",
       "17  LC80150342019224LGN00             None                 NaT 2019-08-12   \n",
       "19  LC80150342019208LGN00             None                 NaT 2019-07-27   \n",
       "20  LC80160332019183LGN00             None                 NaT 2019-07-02   \n",
       "21  LC80160342019183LGN00             None                 NaT 2019-07-02   \n",
       "23  LC80150342018269LGN00             None                 NaT 2018-09-26   \n",
       "28  LC80150342018189LGN00             None                 NaT 2018-07-08   \n",
       "29  LC80160332018180LGN00             None                 NaT 2018-06-29   \n",
       "31  LC80160342017273LGN00             None                 NaT 2017-09-30   \n",
       "33  LC80150342017266LGN00             None                 NaT 2017-09-23   \n",
       "34  LC80150342017250LGN00             None                 NaT 2017-09-07   \n",
       "36  LC80150342017234LGN00             None                 NaT 2017-08-22   \n",
       "39  LC80160332017225LGN00             None                 NaT 2017-08-13   \n",
       "41  LC80160342017161LGN00             None                 NaT 2017-06-10   \n",
       "42  LC80150342017154LGN00             None                 NaT 2017-06-03   \n",
       "43  LC80160342016239LGN01             None                 NaT 2016-08-26   \n",
       "44  LC80150342016200LGN01             None                 NaT 2016-07-18   \n",
       "46  LC80150342016184LGN01             None                 NaT 2016-07-02   \n",
       "\n",
       "     end_date cloud_cover                                     spatial_bounds  \\\n",
       "2  2020-09-22        0.01  [[-79.73837, 37.83651], [-79.73837, 39.95744],...   \n",
       "3  2020-09-22        0.01  [[-80.14489, 36.40826], [-80.14489, 38.52658],...   \n",
       "4  2020-09-06        0.44  [[-79.73071, 37.83621], [-79.73071, 39.95716],...   \n",
       "5  2020-09-06        0.09  [[-80.13726, 36.40795], [-80.13726, 38.52635],...   \n",
       "7  2020-07-20        9.89  [[-80.13223, 36.40817], [-80.13223, 38.52677],...   \n",
       "8  2020-07-13        8.25  [[-78.59079, 36.40771], [-78.59079, 38.52642],...   \n",
       "9  2020-07-04        5.29  [[-80.13743, 36.40785], [-80.13743, 38.52648],...   \n",
       "11 2019-09-29        0.92  [[-78.59598, 36.40802], [-78.59598, 38.52639],...   \n",
       "12 2019-09-20        0.53  [[-79.72973, 37.83664], [-79.72973, 39.95751],...   \n",
       "13 2019-09-20        0.01  [[-80.13634, 36.40788], [-80.13634, 38.52616],...   \n",
       "17 2019-08-12        6.23  [[-78.58301, 36.40793], [-78.58301, 38.52654],...   \n",
       "19 2019-07-27        3.60  [[-78.58056, 36.40779], [-78.58056, 38.52644],...   \n",
       "20 2019-07-02        8.99  [[-79.7307, 37.83649], [-79.7307, 39.95762], [...   \n",
       "21 2019-07-02        3.64  [[-80.13751, 36.40787], [-80.13751, 38.52649],...   \n",
       "23 2018-09-26        7.88  [[-78.58312, 36.40782], [-78.58312, 38.52634],...   \n",
       "28 2018-07-08        0.76  [[-78.5802, 36.40798], [-78.5802, 38.52675], [...   \n",
       "29 2018-06-29        4.54  [[-79.71923, 37.8365], [-79.71923, 39.95777], ...   \n",
       "31 2017-09-30        0.60  [[-80.15024, 36.40789], [-80.15024, 38.52615],...   \n",
       "33 2017-09-23        2.55  [[-78.5961, 36.40789], [-78.5961, 38.52631], [...   \n",
       "34 2017-09-07        1.32  [[-78.58755, 36.4079], [-78.58755, 38.52638], ...   \n",
       "36 2017-08-22        7.14  [[-78.59362, 36.40791], [-78.59362, 38.52644],...   \n",
       "39 2017-08-13        9.57  [[-79.73211, 37.83657], [-79.73211, 39.95754],...   \n",
       "41 2017-06-10        0.44  [[-80.14113, 36.40817], [-80.14113, 38.52672],...   \n",
       "42 2017-06-03        5.56  [[-78.59533, 36.40806], [-78.59533, 38.52674],...   \n",
       "43 2016-08-26        4.82  [[-80.13243, 36.40816], [-80.13243, 38.52659],...   \n",
       "44 2016-07-18        4.73  [[-78.59052, 36.40786], [-78.59052, 38.52652],...   \n",
       "46 2016-07-02        6.01  [[-78.58765, 36.40813], [-78.58765, 38.5268], ...   \n",
       "\n",
       "                                     spatial_coverage  \\\n",
       "2   [[-79.73837, 38.23916], [-77.62342, 37.83651],...   \n",
       "3   [[-80.14489, 36.80681], [-78.06929, 36.40826],...   \n",
       "4   [[-79.73071, 38.23894], [-77.6158, 37.83621], ...   \n",
       "5   [[-80.13726, 36.80658], [-78.06168, 36.40795],...   \n",
       "7   [[-80.13223, 36.80681], [-78.05767, 36.40817],...   \n",
       "8   [[-78.59079, 36.80651], [-76.51531, 36.40771],...   \n",
       "9   [[-80.13743, 36.80654], [-78.0627, 36.40785], ...   \n",
       "11  [[-78.59598, 36.80669], [-76.51954, 36.40802],...   \n",
       "12  [[-79.72973, 38.23923], [-77.61483, 37.83664],...   \n",
       "13  [[-80.13634, 36.80638], [-78.06078, 36.40788],...   \n",
       "17  [[-78.58301, 36.80669], [-76.50726, 36.40793],...   \n",
       "19  [[-78.58056, 36.80655], [-76.50493, 36.40779],...   \n",
       "20  [[-79.7307, 38.23928], [-77.61643, 37.83649], ...   \n",
       "21  [[-80.13751, 36.80656], [-78.06257, 36.40787],...   \n",
       "23  [[-78.58312, 36.8066], [-76.50702, 36.40782], ...   \n",
       "28  [[-78.5802, 36.80708], [-76.50398, 36.40798], ...   \n",
       "29  [[-79.71923, 38.23955], [-77.60454, 37.8365], ...   \n",
       "31  [[-80.15024, 36.80627], [-78.07497, 36.40789],...   \n",
       "33  [[-78.5961, 36.80646], [-76.51989, 36.40789], ...   \n",
       "34  [[-78.58755, 36.80659], [-76.51129, 36.4079], ...   \n",
       "36  [[-78.59362, 36.80665], [-76.51741, 36.40791],...   \n",
       "39  [[-79.73211, 38.23922], [-77.61757, 37.83657],...   \n",
       "41  [[-80.14113, 36.80685], [-78.06613, 36.40817],...   \n",
       "42  [[-78.59533, 36.80695], [-76.51932, 36.40806],...   \n",
       "43  [[-80.13243, 36.80668], [-78.05731, 36.40816],...   \n",
       "44  [[-78.59052, 36.80668], [-76.51465, 36.40786],...   \n",
       "46  [[-78.58765, 36.80702], [-76.51162, 36.40813],...   \n",
       "\n",
       "                                             geometry  start_year  end_year  \\\n",
       "2   POLYGON ((-79.73837 38.23916, -77.62342 37.836...        2020      2020   \n",
       "3   POLYGON ((-80.14489 36.80681, -78.06929 36.408...        2020      2020   \n",
       "4   POLYGON ((-79.73071 38.23894, -77.61580 37.836...        2020      2020   \n",
       "5   POLYGON ((-80.13726 36.80658, -78.06168 36.407...        2020      2020   \n",
       "7   POLYGON ((-80.13223 36.80681, -78.05767 36.408...        2020      2020   \n",
       "8   POLYGON ((-78.59079 36.80651, -76.51531 36.407...        2020      2020   \n",
       "9   POLYGON ((-80.13743 36.80654, -78.06270 36.407...        2020      2020   \n",
       "11  POLYGON ((-78.59598 36.80669, -76.51954 36.408...        2019      2019   \n",
       "12  POLYGON ((-79.72973 38.23923, -77.61483 37.836...        2019      2019   \n",
       "13  POLYGON ((-80.13634 36.80638, -78.06078 36.407...        2019      2019   \n",
       "17  POLYGON ((-78.58301 36.80669, -76.50726 36.407...        2019      2019   \n",
       "19  POLYGON ((-78.58056 36.80655, -76.50493 36.407...        2019      2019   \n",
       "20  POLYGON ((-79.73070 38.23928, -77.61643 37.836...        2019      2019   \n",
       "21  POLYGON ((-80.13751 36.80656, -78.06257 36.407...        2019      2019   \n",
       "23  POLYGON ((-78.58312 36.80660, -76.50702 36.407...        2018      2018   \n",
       "28  POLYGON ((-78.58020 36.80708, -76.50398 36.407...        2018      2018   \n",
       "29  POLYGON ((-79.71923 38.23955, -77.60454 37.836...        2018      2018   \n",
       "31  POLYGON ((-80.15024 36.80627, -78.07497 36.407...        2017      2017   \n",
       "33  POLYGON ((-78.59610 36.80646, -76.51989 36.407...        2017      2017   \n",
       "34  POLYGON ((-78.58755 36.80659, -76.51129 36.407...        2017      2017   \n",
       "36  POLYGON ((-78.59362 36.80665, -76.51741 36.407...        2017      2017   \n",
       "39  POLYGON ((-79.73211 38.23922, -77.61757 37.836...        2017      2017   \n",
       "41  POLYGON ((-80.14113 36.80685, -78.06613 36.408...        2017      2017   \n",
       "42  POLYGON ((-78.59533 36.80695, -76.51932 36.408...        2017      2017   \n",
       "43  POLYGON ((-80.13243 36.80668, -78.05731 36.408...        2016      2016   \n",
       "44  POLYGON ((-78.59052 36.80668, -76.51465 36.407...        2016      2016   \n",
       "46  POLYGON ((-78.58765 36.80702, -76.51162 36.408...        2016      2016   \n",
       "\n",
       "           area  shp_percent  \n",
       "2   4129.867083    45.819210  \n",
       "3   8756.939748    97.154715  \n",
       "4   4110.276417    45.601859  \n",
       "5   8761.611621    97.206547  \n",
       "7   8766.755681    97.263619  \n",
       "8   2745.316903    30.458184  \n",
       "9   8761.698846    97.207515  \n",
       "11  2781.158033    30.855827  \n",
       "12  4100.785398    45.496560  \n",
       "13  8761.744917    97.208026  \n",
       "17  2695.454907    29.904985  \n",
       "19  2679.130377    29.723871  \n",
       "20  4106.674987    45.561903  \n",
       "21  8761.769410    97.208298  \n",
       "23  2696.040403    29.911481  \n",
       "28  2676.366205    29.693204  \n",
       "29  4067.971111    45.132499  \n",
       "31  8751.272784    97.091842  \n",
       "33  2781.823351    30.863208  \n",
       "34  2725.789934    30.241540  \n",
       "36  2765.050416    30.677119  \n",
       "39  4109.944407    45.598176  \n",
       "41  8760.048674    97.189207  \n",
       "42  2775.803655    30.796422  \n",
       "43  8766.156076    97.256966  \n",
       "44  2744.084352    30.444509  \n",
       "46  2725.831395    30.242000  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cville_scene_metadata_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LC08_L2SP_016034_20200922_20201005_02_T1\n",
      "LC08_L2SP_016034_20200906_20200918_02_T1\n",
      "LC08_L2SP_016034_20200720_20210330_02_T1\n",
      "LC08_L2SP_016034_20200704_20200913_02_T1\n",
      "LC08_L2SP_016033_20200922_20201005_02_T1\n",
      "LC08_L2SP_016033_20200906_20200918_02_T1\n",
      "LC08_L2SP_015034_20200713_20200912_02_T1\n",
      "LC08_L2SP_014034_20200722_20200911_02_T1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chasedawson/opt/anaconda3/envs/earth-analytics-python/lib/python3.8/site-packages/rasterstats/main.py:260: UserWarning: Warning: converting a masked element to nan.\n",
      "  feature_stats['nodata'] = float((featmasked == fsrc.nodata).sum())\n"
     ]
    }
   ],
   "source": [
    "cville_stats = compute_zonal_stats(cville_files, {key: cville_shp[key] for key in ['counties', 'tracts', 'blkgps']}, 'cville')\n",
    "easternShore_stats = compute_zonal_stats(easternShore_files, {key: easternShore_shp[key] for key in ['counties', 'tracts', 'blkgps']}, 'easternShore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prop_nodata(nodata, count):\n",
    "    if nodata == 0 and count == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (nodata / (nodata + count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "cville_stats['prop_nodata'] = cville_stats.apply(lambda x: prop_nodata(x['nodata'], x['count']), axis=1)\n",
    "easternShore_stats['prop_nodata'] = easternShore_stats.apply(lambda x: prop_nodata(x['nodata'], x['count']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(stats):\n",
    "    MULTIPLICATIVE_SCALE_FACTOR = 0.00341802\n",
    "    ADDITIVE_SCALE_FACTOR = 149\n",
    "    cols_to_rescale = [\"min\", \"max\", \"mean\", \"median\"]\n",
    "    for col in cols_to_rescale:\n",
    "        stats[col] = stats[col].apply(lambda x: x * MULTIPLICATIVE_SCALE_FACTOR + ADDITIVE_SCALE_FACTOR if type(x) is float else x)\n",
    "        stats[col] = stats[col].apply(lambda x: to_fahrenheit(x) if type(x) is float else x)\n",
    "    return stats\n",
    "\n",
    "def to_fahrenheit(k):\n",
    "    return (k - 273.15) * (9/5) + 32\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "cville_stats = rescale(cville_stats)\n",
    "easternShore_stats = rescale(easternShore_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(stats, metadata, filedata):\n",
    "    entity_to_filename = {x['entityId']: x['filename'] for x in filedata}\n",
    "    metadata = metadata[['entity_id', 'start_date', 'end_date', 'start_year', 'end_year', 'cloud_cover', 'area', 'shp_percent', 'spatial_bounds', 'spatial_coverage']]\n",
    "    metadata = metadata[metadata.start_year == 2020]\n",
    "    metadata['file'] = metadata.entity_id.apply(lambda x: entity_to_filename[x].split('.')[0])\n",
    "    stats = stats.set_index('file').join(metadata.set_index('file'), on=\"file\")\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "cville_stats = merge_data(cville_stats, cville_scene_metadata_f, cville_filedata)\n",
    "easternShore_stats = merge_data(easternShore_stats, easternShore_scene_metadata_f, easternShore_filedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "cville_stats = cville_stats.reset_index()\n",
    "easternShore_stats = easternShore_stats.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break into spatial units and save\n",
    "spatial_units = [\"blkgps\", \"tracts\", \"counties\"]\n",
    "for unit in spatial_units:\n",
    "    cville_stats[cville_stats.spatial_unit == unit].to_csv('landsat8_cville_{unit}.csv'.format(unit = unit))\n",
    "    easternShore_stats[easternShore_stats.spatial_unit == unit].to_csv('landsat8_easternShore_{unit}.csv'.format(unit = unit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "# [Downloading Files From Web Using Python](https://www.tutorialspoint.com/downloading-files-from-web-using-python)\n",
    "# [Jupyter Tips and Tricks](https://chrieke.medium.com/jupyter-tips-and-tricks-994fdddb2057)\n",
    "# [USGS/EROS Inventory Service Documentation (Machine-to-Machine) API](https://m2m.cr.usgs.gov/api/docs/json/#section-overview)\n",
    "# [How are files extracted from a tar file using Python?](https://www.tutorialspoint.com/How-are-files-extracted-from-a-tar-file-using-Python)\n",
    "# [How do I use a scale factor with Landsat Level-2 science products?](https://www.usgs.gov/faqs/how-do-i-use-a-scale-factor-landsat-level-2-science-products?qt-news_science_products=0#qt-news_science_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earth-analytics-python",
   "language": "python",
   "name": "earth-analytics-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
